{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU4REC Grid Search II\n",
    "\n",
    "This notebook tests various GRU4REC models over a set of hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tested Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Hyperparameters in the Spreadsheet.\n",
    "\n",
    "num_epochs = 100\n",
    "lr = 5e-4\n",
    "batch_size = 64\n",
    "reg = 5e-4 # regularization? \n",
    "# bpr_reg: always 0? \n",
    "num_neg_samples = 25\n",
    "train_method = \"alternate\"\n",
    "hidden_dim = 256\n",
    "embedding_dim = 256\n",
    "bert_dim= 768\n",
    "# : ???\n",
    "max_length = 200\n",
    "freeze_plot = False\n",
    "tied = False\n",
    "loss_type = \"BPR\"\n",
    "dilations = [1,2,2,4] # only used for cross entropy\n",
    "# hidden_layers = ???\n",
    "\n",
    "# Hyperparameters not in the Spreadsheet: \n",
    "\n",
    "window = 3\n",
    "dropout= 0\n",
    "k = 10\n",
    "min_len = 10\n",
    "\n",
    "# NextItNet options. \n",
    "hidden_layers = 3\n",
    "model_type = \"feature_add\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing import *\n",
    "from dataset import *\n",
    "from metrics import *\n",
    "from model import *\n",
    "from utils import bert2dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set this to google colab stuff\n",
    "read_filename =\"../data/movielens-1m/ratings.dat\"\n",
    "read_bert_filename = \"../data/movielens-20m/bert_sequence_20m.txt\"\n",
    "read_movie_filename = \"\" #\"movies-1m.csv\"\n",
    "size = \"1m\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Creating DataFrame ==========\n",
      "user_id        6040\n",
      "item_id        3706\n",
      "rating            5\n",
      "timestamp    458455\n",
      "dtype: int64\n",
      "(1000209, 4)\n",
      "Minimum Session Length: 20\n",
      "Maximum Session Length: 2314\n",
      "Average Session Length: 165.60\n",
      "========== Filtering Sessions <= 10  DataFrame ==========\n",
      "user_id        6040\n",
      "item_id        3706\n",
      "rating            5\n",
      "timestamp    458455\n",
      "dtype: int64\n",
      "(1000209, 4)\n",
      "Minimum Session Length: 20\n",
      "Maximum Session Length: 2314\n",
      "Average Session Length: 165.60\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n"
     ]
    }
   ],
   "source": [
    "# ------------------Data Initialization----------------------#\n",
    "# convert .dat file to time-sorted pandas dataframe\n",
    "ml_1m = create_df(read_filename, size=size)\n",
    "\n",
    "# remove users who have session lengths less than min_len\n",
    "ml_1m = filter_df(ml_1m, item_min=min_len)\n",
    "\n",
    "if read_movie_filename != \"\":\n",
    "    ml_movie_df = create_movie_df(read_movie_filename,size=size)\n",
    "    ml_movie_df = convert_genres(ml_movie_df)\n",
    "    \n",
    "    # initialize reset object\n",
    "    reset_object = reset_df()\n",
    "    \n",
    "    # map all user ids, item ids, and genres to range 0 - number of users/items/genres\n",
    "    ml_1m,ml_movie_df = reset_object.fit_transform(ml_1m,ml_movie_df)\n",
    "    \n",
    "    # value that padded genre tokens shall take\n",
    "    pad_genre_token = reset_object.genre_enc.transform([\"NULL\"]).item()\n",
    "    \n",
    "    genre_dim = len(np.unique(np.concatenate(ml_movie_df.genre))) - 1\n",
    "\n",
    "else:\n",
    "    # initialize reset object\n",
    "    reset_object = reset_df()\n",
    "    \n",
    "    # map all user ids and item ids to range 0 - Number of Users/Items \n",
    "    # i.e. [1,7,5] -> [0,2,1]\n",
    "    ml_1m = reset_object.fit_transform(ml_1m)\n",
    "    \n",
    "    pad_genre_token = None\n",
    "    ml_movie_df = None\n",
    "    genre_dim = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Reading .txt file with all item id and embeddings ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31/6040 [00:00<00:19, 304.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Creating User Histories ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:11<00:00, 529.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# ------------------Data Initialization----------------------#\n",
    "# how many unique users, items, ratings and timestamps are there\n",
    "n_users, n_items, n_ratings, n_timestamp = ml_1m.nunique()\n",
    "\n",
    "# value that padded tokens shall take\n",
    "pad_token = n_items\n",
    "\n",
    "# the output dimension for softmax layer\n",
    "output_dim = n_items\n",
    "\n",
    "# get the item id : bert plot embedding dictionary\n",
    "if bert_dim != 0:\n",
    "    feature_embed = bert2dict(bert_filename=read_bert_filename)\n",
    "    \n",
    "# create a dictionary of every user's session (history)\n",
    "# i.e. {user: [user clicks]}\n",
    "user_history = create_user_history(ml_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:00<00:00, 105690.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Splitting User Histories into Train, Validation, and Test Splits ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# split data by leave-one-out strategy\n",
    "# have train dictionary {user: [last 41 items prior to last 2 items in user session]}\n",
    "# have val dictionary {user: [last 41 items prior to last item in user session]}\n",
    "# have test dictionary {user: [last 41 items]}\n",
    "# i.e. if max_length = 4, [1,2,3,4,5,6] -> [1,2,3,4] , [2,3,4,5] , [3,4,5,6]\n",
    "train_history,val_history,test_history = train_val_test_split(user_history,max_length=max_length)\n",
    "\n",
    "# initialize the train,validation, and test pytorch dataset objects\n",
    "# eval pads all items except last token to predict\n",
    "train_dataset = GRUDataset(train_history,genre_df=ml_movie_df,mode='train',max_length=max_length,\n",
    "                           pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
    "val_dataset = GRUDataset(val_history,genre_df=ml_movie_df,mode='eval',max_length=max_length,\n",
    "                         pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
    "test_dataset = GRUDataset(test_history,genre_df=ml_movie_df,mode='eval',max_length=max_length,\n",
    "                          pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
    "\n",
    "# create the train,validation, and test pytorch dataloader objects\n",
    "train_dl = DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
    "val_dl = DataLoader(val_dataset,batch_size=64)\n",
    "test_dl = DataLoader(test_dataset,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_type, device, embedding_dim=embedding_dim,\n",
    "             hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "             genre_dim=genre_dim,\n",
    "             batch_first=True,\n",
    "             max_length=max_length,\n",
    "             pad_token=pad_token,\n",
    "             pad_genre_token=pad_genre_token,\n",
    "             bert_dim=bert_dim,\n",
    "             tied = tied,\n",
    "             dropout=dropout): \n",
    "    # initialize gru4rec model with arguments specified earlier\n",
    "    if model_type == \"feature_add\":\n",
    "        model = gru4recF(embedding_dim=embedding_dim,\n",
    "             hidden_dim=hidden_dim,\n",
    "             output_dim=output_dim,\n",
    "             genre_dim=genre_dim,\n",
    "             batch_first=True,\n",
    "             max_length=max_length,\n",
    "             pad_token=pad_token,\n",
    "             pad_genre_token=pad_genre_token,\n",
    "             bert_dim=bert_dim,\n",
    "             tied = tied,\n",
    "             dropout=dropout)\n",
    "\n",
    "\n",
    "    if model_type == \"feature_concat\":\n",
    "        model = gru4recFC(embedding_dim=embedding_dim,\n",
    "             hidden_dim=hidden_dim,\n",
    "             output_dim=output_dim,\n",
    "             genre_dim=genre_dim,\n",
    "             batch_first=True,\n",
    "             max_length=max_length,\n",
    "             pad_token=pad_token,\n",
    "             pad_genre_token=pad_genre_token,\n",
    "             bert_dim=bert_dim,\n",
    "             tied = tied,\n",
    "             dropout=dropout)\n",
    "\n",
    "    if model_type == \"vanilla\":\n",
    "        model = gru4rec_vanilla(hidden_dim=hidden_dim,\n",
    "                            output_dim=output_dim,\n",
    "                            batch_first=True,\n",
    "                            max_length=max_length,\n",
    "                            pad_token=pad_token,\n",
    "                            tied=tied,\n",
    "                            embedding_dim=embedding_dim,\n",
    "                           device=device)\n",
    "\n",
    "    if model_type ==\"feature_only\":\n",
    "        model = gru4rec_feature(hidden_dim=hidden_dim,\n",
    "                            output_dim=output_dim,\n",
    "                            batch_first=True,\n",
    "                            max_length=max_length,\n",
    "                            pad_token=pad_token,\n",
    "                            bert_dim=bert_dim)\n",
    "\n",
    "    if model_type == \"conv\":\n",
    "        model = gru4rec_conv(embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 batch_first=True,\n",
    "                 max_length=200,\n",
    "                 pad_token=0,\n",
    "                 dropout=0,\n",
    "                 window=window,\n",
    "                 tied=tied)\n",
    "    \n",
    "    if model_type == \"nextitnet\":\n",
    "        model = NextItNet(embedding_dim=embedding_dim,\n",
    "                      output_dim=output_dim,\n",
    "                      hidden_layers=hidden_layers,\n",
    "                      dilations=dilations,\n",
    "                      pad_token=n_items,\n",
    "                      max_len=max_length)\n",
    "    \n",
    "    if bert_dim != 0:\n",
    "        model.init_weight(reset_object,feature_embed)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move tihs somewhere\n",
    "if freeze_plot and bert_dim != 0:\n",
    "    model.plot_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_loss_function(loss_type):\n",
    "    if loss_type == \"XE\":\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index=n_items)\n",
    "    elif loss_type == \"BPR\":\n",
    "        loss_fn = BPRLoss(user_history = user_history,\n",
    "                      n_items = n_items, \n",
    "                      df = ml_1m,\n",
    "                      device = device, \n",
    "                      samples=num_neg_samples)\n",
    "    elif loss_type == \"BPR_MAX\":\n",
    "        loss_fn = BPRMaxLoss(user_history = user_history,\n",
    "                      n_items = n_items, \n",
    "                      df = ml_1m,\n",
    "                      device = device,\n",
    "                      reg = 2, # TODO: why is this a 2 and not \"reg\"? \n",
    "                      samples=num_neg_samples)\n",
    "    else: \n",
    "        raise ValueError(\"Unknown Loss Type.\")\n",
    "        \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Creating Hit@10 Metric Object ==========\n"
     ]
    }
   ],
   "source": [
    "# Initialize Metric Object\n",
    "Recall_Object = Recall_E_prob(ml_1m,user_history,n_users,n_items,k=k,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------Training Initialization----------------------#\n",
    "\n",
    "\n",
    "def record_best_tuple(max_train, max_validation, max_testing, new_train, new_validation, new_testing): \n",
    "    if max_validation[0] < new_validation[0]: \n",
    "        return new_train, new_validation, new_testing\n",
    "    return max_train, max_validation, max_testing\n",
    "\n",
    "\n",
    "def train_model(model, num_epochs, loss_fn, loss_type, train_method, tied): \n",
    "    max_train_hit = (0,0,0)\n",
    "    max_val_hit = (0,0,0)\n",
    "    max_test_hit = (0,0,0)\n",
    "    max_train_ndcg = (0,0,0)\n",
    "    max_val_ndcg = (0,0,0)\n",
    "    max_test_ndcg = (0,0,0)\n",
    "    max_train_mrr = 0\n",
    "    max_val_mrr = 0\n",
    "    max_test_mrr = 0\n",
    "    \n",
    "    if train_method != \"normal\":\n",
    "        optimizer_features = torch.optim.Adam([param for name, param in model.named_parameters() \n",
    "                                               if ((\"movie\" not in name) or (\"plot_embedding\" in name) \n",
    "                                               or (\"genre\" in name))],\n",
    "                                              lr=lr/10,weight_decay=reg)\n",
    "        optimizer_ids = torch.optim.Adam([param for name, param in model.named_parameters() \n",
    "                                          if (\"plot\" not in name) and (\"genre\" not in name)],\n",
    "                                         lr=lr,weight_decay=reg)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=reg)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"=\"*20,\"Epoch {}\".format(epoch+1),\"=\"*20)\n",
    "    \n",
    "        model.train()  \n",
    "    \n",
    "        running_loss = 0\n",
    "\n",
    "        for j, data in enumerate(tqdm(train_dl,position=0,leave=True)):\n",
    "            if train_method != \"normal\":\n",
    "                optimizer_features.zero_grad()\n",
    "                optimizer_ids.zero_grad()\n",
    "            else: \n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "            if genre_dim != 0:            \n",
    "                inputs, genre_inputs, labels, x_lens,uid = data\n",
    "                outputs = model(x=inputs.to(device),x_lens=x_lens.squeeze().tolist(),\n",
    "                                x_genre=genre_inputs.to(device))\n",
    "            else:\n",
    "                inputs,labels,x_lens,uid = data \n",
    "                outputs = model(x=inputs.to(device),x_lens=x_lens.squeeze().tolist())\n",
    "       \n",
    "            if tied:\n",
    "                outputs_ignore_pad = outputs[:,:,:-1]\n",
    "                if loss_type == \"XE\":\n",
    "                    loss = loss_fn(outputs_ignore_pad.view(-1,outputs_ignore_pad.size(-1)),labels.view(-1).to(device))\n",
    "                elif loss_type == \"BPR\" or loss_type == \"BPR_MAX\":\n",
    "                    loss = loss_fn(outputs,labels.to(device),x_lens,uid)\n",
    "            else:\n",
    "                if loss_type == \"XE\":\n",
    "                    loss = loss_fn(outputs.view(-1,outputs.size(-1)),labels.view(-1).to(device))\n",
    "                elif loss_type == \"BPR\" or loss_type == \"BPR_MAX\":   \n",
    "                    loss = loss_fn(outputs,labels.to(device),x_lens,uid)\n",
    "\n",
    "            loss.backward()\n",
    "        \n",
    "        \n",
    "            if train_method != \"normal\":\n",
    "                if train_method == \"interleave\":\n",
    "                    # interleave on the epochs\n",
    "                    if (j+1) % 2 == 0:\n",
    "                        optimizer_features.step()\n",
    "                    else:\n",
    "                        optimizer_ids.step()\n",
    "\n",
    "                elif train_method == \"nate\":\n",
    "                    if (epoch+1) % 2 == 0:\n",
    "                        optimizer_features.step()\n",
    "                    else:\n",
    "                        optimizer_ids.step()\n",
    "                    \n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.detach().cpu().item()\n",
    "\n",
    "        del outputs\n",
    "    \n",
    "        if torch.cuda.is_available(): \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    \n",
    "        training_hit, training_ndcg, train_mrr = Recall_Object(model,train_dl,\"train\")\n",
    "        validation_hit, validation_ndcg, validation_mrr = Recall_Object(model,val_dl,\"validation\")\n",
    "        testing_hit, testing_ndcg, testing_mrr = Recall_Object(model,test_dl,\"test\")\n",
    "    \n",
    "        # Record the best metrics that our model obtained\n",
    "        max_train_ndcg, max_val_ndcg, max_test_ndcg = record_best_tuple(\n",
    "            max_train_ndcg, max_val_ndcg, max_test_ndcg, \n",
    "            training_ndcg, validation_ndcg, testing_ndcg)\n",
    "        max_train_hit, max_val_hit, max_test_hit = record_best_tuple(\n",
    "            max_train_hit, max_val_hit, max_test_hit, \n",
    "            training_hit, validation_hit, testing_hit)\n",
    "        if validation_mrr > max_val_mrr: \n",
    "            max_train_mrr = training_mrr\n",
    "            max_val_mrr = validation_mrr\n",
    "            max_test_mrr = testing_mrr\n",
    "    \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        print(\"Training Loss: {:.5f}\".format(running_loss/len(train_dl)))\n",
    "        print(\"Train Hits \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*training_hit))\n",
    "        print(\"Train ndcg \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*training_ndcg))\n",
    "        print(\"Valid Hits \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*validation_hit))\n",
    "        print(\"Valid ndcg \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*validation_ndcg))\n",
    "        print(\"Test Hits \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*testing_hit))\n",
    "        print(\"Test ndcg \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*testing_ndcg))\n",
    "\n",
    "        \n",
    "    print(\"=\"*100)\n",
    "    print(\"Maximum Training Hit \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*max_train_hit))\n",
    "    print(\"Maximum Validation Hit \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*max_val_hit))\n",
    "    print(\"Maximum Testing Hit \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*max_test_hit))\n",
    "    return ((max_train_hit, max_val_hit, max_test_hit), \n",
    "            (max_train_ndcg, max_test_ndcg, max_val_ndcg), \n",
    "            (max_train_mrr, max_test_mrr, max_val_mrr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPR\n",
      "3706\n",
      "==================== Epoch 1 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [18:55<00:00, 11.95s/it]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'max_train_ndcg' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-77f7aee5b131>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-f82320ecb496>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, num_epochs, loss_fn, loss_type, train_method, tied)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;31m# Record the best metrics that our model obtained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         max_train_ndcg, max_val_ndcg, max_test_ndcg = record_best_tuple(\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[0mmax_train_ndcg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_val_ndcg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_test_ndcg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             training_ndcg, validation_ndcg, testing_ndcg)\n\u001b[0;32m    103\u001b[0m         max_train_hit, max_val_hit, max_test_hit = record_best_tuple(\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'max_train_ndcg' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "model = initialize_model(model_type=model_type, device=device, \n",
    "            embedding_dim=embedding_dim,\n",
    "             hidden_dim=hidden_dim, \n",
    "            output_dim=output_dim,\n",
    "             genre_dim=genre_dim,\n",
    "             batch_first=True,\n",
    "             max_length=max_length,\n",
    "             pad_token=pad_token,\n",
    "             pad_genre_token=pad_genre_token,\n",
    "             bert_dim=bert_dim,\n",
    "             tied = tied,\n",
    "             dropout=dropout)\n",
    "\n",
    "print(loss_type)\n",
    "\n",
    "loss_function = initialize_loss_function(loss_type)\n",
    "assert loss_function is not None\n",
    "print(train_model(model, 2, loss_function, loss_type, train_method, tied))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
