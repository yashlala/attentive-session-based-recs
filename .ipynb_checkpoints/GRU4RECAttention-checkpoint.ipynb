{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing import *\n",
    "from dataset import *\n",
    "from metrics import *\n",
    "from model import *\n",
    "from utils import bert2dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class gru4recFC_encoder(nn.Module):\n",
    "#     \"\"\"\n",
    "#     embedding dim: the dimension of the item-embedding look-up table\n",
    "#     hidden_dim: the dimension of the hidden state of the GRU-RNN\n",
    "#     batch_first: whether the batch dimension should be the first dimension of input to GRU-RNN\n",
    "#     output_dim: the output dimension of the last fully connected layer\n",
    "#     max_length: the maximum session length for any user, used for packing/padding input to GRU-RNN\n",
    "#     pad_token: the value that pad tokens should be set to for GRU-RNN and item embedding\n",
    "#     bert_dim: the dimension of the feature-embedding look-up table\n",
    "#     ... to do add all comments ... \n",
    "#     \"\"\"\n",
    "#     def __init__(self,embedding_dim,hidden_dim,output_dim,genre_dim=0,batch_first=True,max_length=200,bert_dim=0,tied=False,dropout=0):\n",
    "#         super(gru4recFC_encoder,self).__init__()\n",
    "        \n",
    "#         self.batch_first =batch_first\n",
    "        \n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.hidden_dim =hidden_dim\n",
    "#         self.output_dim =output_dim\n",
    "#         self.genre_dim = genre_dim\n",
    "#         self.bert_dim = bert_dim\n",
    "\n",
    "#         self.max_length = max_length\n",
    "# #         self.pad_token = pad_token\n",
    "# #         self.pad_genre_token = pad_genre_token\n",
    "#         self.tied = tied\n",
    "        \n",
    "#         self.dropout = dropout\n",
    "    \n",
    "#         if self.tied:\n",
    "#             self.hidden_dim = embedding_dim\n",
    "#         # initialize item-id lookup table\n",
    "#         # add 1 to output dimension because we have to add a pad token\n",
    "#         self.movie_embedding = nn.Embedding(output_dim,embedding_dim)\n",
    "        \n",
    "#         #  initialize plot lookup table\n",
    "#         # add 1 to output dimensino because we have to add a pad token\n",
    "#         if bert_dim != 0:\n",
    "#             self.plot_embedding = nn.Embedding(output_dim,bert_dim)\n",
    "#             #self.plot_embedding.requires_grad_(requires_grad=False)\n",
    "#             #self.plot_embedding = torch.ones(output_dim+1,bert_dim).cuda() #nn.Embedding(output_dim+1,bert_dim,padding_idx=pad_token)\n",
    "#             #self.plot_embedding[pad_token,:] = 0\n",
    "        \n",
    "#         if genre_dim != 0:\n",
    "#             self.genre_embedding = nn.Embedding(genre_dim,embedding_dim)\n",
    "\n",
    "#         self.projection_layer = nn.Linear(bert_dim+embedding_dim+genre_dim,embedding_dim)\n",
    "        \n",
    "#         self.encoder_layer = nn.GRU(embedding_dim,self.hidden_dim,batch_first=self.batch_first,dropout=self.dropout)\n",
    "\n",
    "#         # add 1 to the output dimension because we have to add a pad token\n",
    "# #         if not self.tied:\n",
    "# #             self.output_layer = nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "# #         if self.tied:\n",
    "# #             self.output_layer = nn.Linear(hidden_dim,output_dim+1)\n",
    "# #             self.output_layer.weight = self.movie_embedding.weight\n",
    "            \n",
    "    \n",
    "#     def forward(self,x,x_lens,x_genre=None,pack=True):\n",
    "#         # add the plot embedding and movie embedding\n",
    "#         # do I add non-linearity or not? ... \n",
    "#         # concatenate or not? ...\n",
    "#         # many questions ...\n",
    "#         if (self.bert_dim != 0) and (self.genre_dim != 0):\n",
    "#             x = torch.cat( (self.movie_embedding(x),self.plot_embedding(x),self.genre_embedding(x_genre).sum(2)) , 2)\n",
    "#         elif (self.bert_dim != 0) and (self.genre_dim == 0):\n",
    "#             x = torch.cat( (self.movie_embedding(x),self.plot_embedding(x) ) , 2)\n",
    "#         elif (self.bert_dim == 0) and (self.genre_dim != 0):\n",
    "#             x = torch.cat( (self.movie_embedding(x),self.genre_embedding(x_genre).sum(2)) , 2)\n",
    "#         else:\n",
    "#             x = self.movie_embedding(x)\n",
    "        \n",
    "#         x = self.projection_layer(x)\n",
    "#         x = F.leaky_relu(x)\n",
    "                    \n",
    "# #         if pack:\n",
    "# #             x = pack_padded_sequence(x,x_lens,batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "# #         output_packed,_ = self.encoder_layer(x)        \n",
    "#         x, hidden = self.encoder_layer(x)\n",
    "        \n",
    "# #         if pack:\n",
    "# #             x, _ = pad_packed_sequence(output_packed, batch_first=self.batch_first,total_length=self.max_length,padding_value=self.pad_token)\n",
    "            \n",
    "# #         x = self.output_layer(x)\n",
    "                \n",
    "#         return x, hidden\n",
    "    \n",
    "#     def init_weight(self,reset_object,feature_embed):\n",
    "#         for (item_id,embedding) in feature_embed.items():\n",
    "#             if item_id not in reset_object.item_enc.classes_:\n",
    "#                 continue\n",
    "#             item_id = reset_object.item_enc.transform([item_id]).item()\n",
    "#             self.plot_embedding.weight.data[item_id,:] = torch.DoubleTensor(embedding)\n",
    "            \n",
    "            \n",
    "class gru4recF_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    embedding dim: the dimension of the item-embedding look-up table\n",
    "    hidden_dim: the dimension of the hidden state of the GRU-RNN\n",
    "    batch_first: whether the batch dimension should be the first dimension of input to GRU-RNN\n",
    "    output_dim: the output dimension of the last fully connected layer\n",
    "    max_length: the maximum session length for any user, used for packing/padding input to GRU-RNN\n",
    "    pad_token: the value that pad tokens should be set to for GRU-RNN and item embedding\n",
    "    bert_dim: the dimension of the feature-embedding look-up table\n",
    "    ... to do add all comments ... \n",
    "    \"\"\"\n",
    "    def __init__(self,embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 genre_dim=0,\n",
    "                 batch_first=True,\n",
    "                 max_length=200,\n",
    "                 pad_token=0,\n",
    "                 pad_genre_token=0,\n",
    "                 bert_dim=0,\n",
    "                 dropout=0,\n",
    "                 tied=False):\n",
    "        \n",
    "        super(gru4recF_encoder,self).__init__()\n",
    "        \n",
    "        self.batch_first =batch_first\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim =hidden_dim\n",
    "        self.output_dim =output_dim\n",
    "        self.genre_dim = genre_dim\n",
    "        self.bert_dim = bert_dim\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_genre_token = pad_genre_token\n",
    "        \n",
    "        self.tied = tied\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        if self.tied:\n",
    "            self.hidden_dim = embedding_dim\n",
    "    \n",
    "        # initialize item-id lookup table\n",
    "        # add 1 to output dimension because we have to add a pad token\n",
    "        self.movie_embedding = nn.Embedding(output_dim+1,embedding_dim,padding_idx=pad_token)\n",
    "        \n",
    "        #  initialize plot lookup table\n",
    "        # add 1 to output dimensino because we have to add a pad token\n",
    "        if bert_dim != 0:\n",
    "            self.plot_embedding = nn.Embedding(output_dim+1,bert_dim,padding_idx=pad_token)\n",
    "            #self.plot_embedding.requires_grad_(requires_grad=False)\n",
    "            #self.plot_embedding = torch.ones(output_dim+1,bert_dim).cuda() #nn.Embedding(output_dim+1,bert_dim,padding_idx=pad_token)\n",
    "            #self.plot_embedding[pad_token,:] = 0\n",
    "            \n",
    "            # project plot embedding to same dimensionality as movie embedding\n",
    "            self.plot_projection = nn.Linear(bert_dim,embedding_dim)\n",
    "                    \n",
    "        if genre_dim != 0:\n",
    "            self.genre_embedding = nn.Embedding(genre_dim+1,embedding_dim,padding_idx=pad_genre_token)\n",
    "\n",
    "\n",
    "        self.encoder_layer = nn.GRU(embedding_dim,self.hidden_dim,batch_first=self.batch_first,dropout=self.dropout)\n",
    "\n",
    "#         # add 1 to the output dimension because we have to add a pad token\n",
    "#         if not self.tied:\n",
    "#             self.output_layer = nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "#         if self.tied:\n",
    "#             self.output_layer = nn.Linear(hidden_dim,output_dim+1)\n",
    "#             self.output_layer.weight = self.movie_embedding.weight\n",
    "    \n",
    "    def forward(self,x,x_lens,x_genre=None,pack=True):\n",
    "        # add the plot embedding and movie embedding\n",
    "        # do I add non-linearity or not? ... \n",
    "        # concatenate or not? ...\n",
    "        # many questions ...\n",
    "        if (self.bert_dim != 0) and (self.genre_dim != 0):\n",
    "            x = self.movie_embedding(x) + self.plot_projection(F.leaky_relu(self.plot_embedding(x))) + self.genre_embedding(x_genre).sum(2)\n",
    "        elif (self.bert_dim != 0) and (self.genre_dim == 0):\n",
    "            x = self.movie_embedding(x) + self.plot_projection(F.leaky_relu(self.plot_embedding(x)))\n",
    "        elif (self.bert_dim == 0) and (self.genre_dim != 0):\n",
    "            x = self.movie_embedding(x) + self.genre_embedding(x_genre).sum(2)\n",
    "        else:\n",
    "            x = self.movie_embedding(x)\n",
    "        \n",
    "#         print(\"Embedder Dimension: \")\n",
    "#         print(x.size())\n",
    "        \n",
    "        if pack:\n",
    "            x = pack_padded_sequence(x,x_lens,batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        output_packed,hidden_state = self.encoder_layer(x) \n",
    "        \n",
    "        if pack:\n",
    "            x, _ = pad_packed_sequence(output_packed, batch_first=self.batch_first,total_length=self.max_length,padding_value=self.pad_token)\n",
    "        \n",
    "        return x,hidden_state\n",
    "        \n",
    "#         if pack:\n",
    "#             x, _ = pad_packed_sequence(output_packed, batch_first=self.batch_first,total_length=self.max_length,padding_value=self.pad_token)\n",
    "            \n",
    "#         x = self.output_layer(x)\n",
    "        \n",
    "                \n",
    "#         return x\n",
    "    \n",
    "    def init_weight(self,reset_object,feature_embed):\n",
    "        for (item_id,embedding) in feature_embed.items():\n",
    "            if item_id not in reset_object.item_enc.classes_:\n",
    "                continue\n",
    "            item_id = reset_object.item_enc.transform([item_id]).item()\n",
    "            self.plot_embedding.weight.data[item_id,:] = torch.DoubleTensor(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 200\n",
    "# class gru4recFC_decoder(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "#         super(gru4recFC_decoder, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.dropout_p = dropout_p\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "#         self.dropout = nn.Dropout(self.dropout_p)\n",
    "#         self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "#     def forward(self, input, hidden, encoder_outputs):\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         embedded = self.dropout(embedded)\n",
    "\n",
    "#         attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "\n",
    "#         output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "#         return output, hidden, attn_weights\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "# # class EncoderRNN(nn.Module):\n",
    "# #     def __init__(self, input_size, hidden_size):\n",
    "# #         super(EncoderRNN, self).__init__()\n",
    "# #         self.hidden_size = hidden_size\n",
    "\n",
    "# #         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "# #         self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "# #     def forward(self, input, hidden):\n",
    "# #         embedded = self.embedding(input).view(1, 1, -1)\n",
    "# #         output = embedded\n",
    "# #         output, hidden = self.gru(output, hidden)\n",
    "# #         return output, hidden\n",
    "\n",
    "# #     def initHidden(self):\n",
    "# #         return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class gru4recF_decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, dropout=0, max_length=MAX_LENGTH):\n",
    "        super(gru4recF_decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_dim+1, self.hidden_dim)\n",
    "        self.attn = nn.Linear(self.hidden_dim * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.gru = nn.GRU(self.hidden_dim, self.hidden_dim)\n",
    "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, input.size()[1], -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "#         print(\"Pre-Embedded Tensor: \")\n",
    "#         print(input[0])\n",
    "        \n",
    "#         print(\"Embedded Dimension: \")\n",
    "#         print(embedded.size())\n",
    "        \n",
    "#         print(\"Hidden Dimension: \")\n",
    "#         print(hidden.size())\n",
    "\n",
    "#         print(\"Embedded Tensor: \")\n",
    "#         print(embedded[0])\n",
    "        \n",
    "#         print(\"Hidden Tensor: \")\n",
    "#         print(hidden[0])\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        \n",
    "#         print(\"Attention Dimension: \")\n",
    "#         print(attn_weights.size())\n",
    "        \n",
    "#         print(\"Encoder Outputs Dimension: \")\n",
    "#         print(encoder_outputs.size())\n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n",
    "                                 encoder_outputs).squeeze(1)\n",
    "        \n",
    "#         print(\"Attention Applied Dimension: \")\n",
    "#         print(attn_applied.size())\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = self.out(output[0])\n",
    "        \n",
    "#         print(\"Output Dimension: \")\n",
    "#         print(output.size())\n",
    "        \n",
    "#         print(\"New Hidden Dimension: \")\n",
    "#         print(hidden.size())\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class gru4recF_attention(nn.Module):\n",
    "#         def __init__(self,embedding_dim,\n",
    "#                  hidden_dim,\n",
    "#                  output_dim,\n",
    "#                  genre_dim=0,\n",
    "#                  batch_first=True,\n",
    "#                  max_length=200,\n",
    "#                  pad_token=0,\n",
    "#                  pad_genre_token=0,\n",
    "#                  bert_dim=0,\n",
    "#                  dropout=0,\n",
    "#                  tied=False):\n",
    "#             self.bert_dim = bert_dim\n",
    "#             self.genre_dim = genre_dim\n",
    "            \n",
    "#             self.model = gru4recF_encoder(embedding_dim=embedding_dim,\n",
    "#              hidden_dim=hidden_dim,\n",
    "#              output_dim=output_dim,\n",
    "#              genre_dim=genre_dim,\n",
    "#              batch_first=True,\n",
    "#              max_length=max_length,\n",
    "#              bert_dim=bert_dim,\n",
    "#              tied = tied,\n",
    "#              dropout=dropout)\n",
    "\n",
    "#             self.modelD = gru4recF_decoder(hidden_dim=hidden_dim, output_dim=output_dim, dropout=0, max_length=max_length)\n",
    "            \n",
    "#         def forward(self,x,x_lens,labels,x_genre=None,pack=True):\n",
    "#             encoder_outputs, hidden_states = self.model(x=inputs.to(device),x_lens=x_lens.squeeze().tolist())\n",
    "#             decoder_inputs = inputs[:,0].view(1,-1).to(device)\n",
    "#             decoder_hidden = hidden_states\n",
    "            \n",
    "#             outputs = torch.zeros(inputs.size()[0],max_length,output_dim, device=device)\n",
    "            \n",
    "#             for i in range(max_length):\n",
    "#                 decoder_outputs, decoder_hidden, decoder_attention = self.modelD(decoder_inputs, decoder_hidden, encoder_outputs)\n",
    "#                 outputs[:,i,:] = decoder_outputs\n",
    "#                 decoder_inputs = labels[:,i].view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gru4recF_attention(nn.Module):\n",
    "    \"\"\"\n",
    "    embedding dim: the dimension of the item-embedding look-up table\n",
    "    hidden_dim: the dimension of the hidden state of the GRU-RNN\n",
    "    batch_first: whether the batch dimension should be the first dimension of input to GRU-RNN\n",
    "    output_dim: the output dimension of the last fully connected layer\n",
    "    max_length: the maximum session length for any user, used for packing/padding input to GRU-RNN\n",
    "    pad_token: the value that pad tokens should be set to for GRU-RNN and item embedding\n",
    "    bert_dim: the dimension of the feature-embedding look-up table\n",
    "    ... to do add all comments ... \n",
    "    \"\"\"\n",
    "    def __init__(self,embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 genre_dim=0,\n",
    "                 batch_first=True,\n",
    "                 max_length=200,\n",
    "                 pad_token=0,\n",
    "                 pad_genre_token=0,\n",
    "                 bert_dim=0,\n",
    "                 dropout=0,\n",
    "                 tied=False):\n",
    "        \n",
    "        super(gru4recF_attention,self).__init__()\n",
    "        \n",
    "        self.batch_first =batch_first\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim =hidden_dim\n",
    "        self.output_dim =output_dim\n",
    "        self.genre_dim = genre_dim\n",
    "        self.bert_dim = bert_dim\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_genre_token = pad_genre_token\n",
    "        \n",
    "        self.tied = tied\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        if self.tied:\n",
    "            self.hidden_dim = embedding_dim\n",
    "    \n",
    "        # initialize item-id lookup table\n",
    "        # add 1 to output dimension because we have to add a pad token\n",
    "        self.movie_embedding = nn.Embedding(output_dim+1,embedding_dim,padding_idx=pad_token)\n",
    "        \n",
    "        #  initialize plot lookup table\n",
    "        # add 1 to output dimensino because we have to add a pad token\n",
    "        if bert_dim != 0:\n",
    "            self.plot_embedding = nn.Embedding(output_dim+1,bert_dim,padding_idx=pad_token)\n",
    "            #self.plot_embedding.requires_grad_(requires_grad=False)\n",
    "            #self.plot_embedding = torch.ones(output_dim+1,bert_dim).cuda() #nn.Embedding(output_dim+1,bert_dim,padding_idx=pad_token)\n",
    "            #self.plot_embedding[pad_token,:] = 0\n",
    "            \n",
    "            # project plot embedding to same dimensionality as movie embedding\n",
    "            self.plot_projection = nn.Linear(bert_dim,embedding_dim)\n",
    "                    \n",
    "        if genre_dim != 0:\n",
    "            self.genre_embedding = nn.Embedding(genre_dim+1,embedding_dim,padding_idx=pad_genre_token)\n",
    "\n",
    "\n",
    "        self.encoder_layer = nn.GRU(embedding_dim,self.hidden_dim,batch_first=self.batch_first,dropout=self.dropout)\n",
    "\n",
    "#         # add 1 to the output dimension because we have to add a pad token\n",
    "        if not self.tied:\n",
    "            self.output_layer = nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "        if self.tied:\n",
    "            self.output_layer = nn.Linear(hidden_dim,output_dim+1)\n",
    "            self.output_layer.weight = self.movie_embedding.weight\n",
    "    \n",
    "    def forward(self,x,x_lens,x_genre=None,pack=True):\n",
    "        # add the plot embedding and movie embedding\n",
    "        # do I add non-linearity or not? ... \n",
    "        # concatenate or not? ...\n",
    "        # many questions ...\n",
    "        batch_size = x.size()[0]\n",
    "        if (self.bert_dim != 0) and (self.genre_dim != 0):\n",
    "            x = self.movie_embedding(x) + self.plot_projection(F.leaky_relu(self.plot_embedding(x))) + self.genre_embedding(x_genre).sum(2)\n",
    "        elif (self.bert_dim != 0) and (self.genre_dim == 0):\n",
    "            x = self.movie_embedding(x) + self.plot_projection(F.leaky_relu(self.plot_embedding(x)))\n",
    "        elif (self.bert_dim == 0) and (self.genre_dim != 0):\n",
    "            x = self.movie_embedding(x) + self.genre_embedding(x_genre).sum(2)\n",
    "        else:\n",
    "            x = self.movie_embedding(x)\n",
    "        \n",
    "#         print(\"Embedder Dimension: \")\n",
    "#         print(x.size())\n",
    "        \n",
    "        if pack:\n",
    "            x = pack_padded_sequence(x,x_lens,batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        output_packed,hidden_state = self.encoder_layer(x) \n",
    "        \n",
    "        if pack:\n",
    "            encoder_states, _ = pad_packed_sequence(output_packed, batch_first=self.batch_first,total_length=self.max_length,padding_value=self.pad_token)\n",
    "        \n",
    "        # CCs = BS x MS x 2HS\n",
    "        combined_contexts = torch.zeros(batch_size,max_length,self.hidden_dim*2)\n",
    "        \n",
    "        for t in range(max_length):\n",
    "            # CF = BS x (t+1) x HS\n",
    "            context_frame = encoder_states[:,:t+1,:]\n",
    "            # CH = BS x HS x 1\n",
    "            current_hidden = encoder_states[:,t,:].squeeze(1).unsqueeze(2)\n",
    "            # AS = BS x (t+1) x 1\n",
    "            attention_score = torch.bmm(context_frame,current_hidden)\n",
    "            # CFT = BS x HS x (t+1)\n",
    "            context_frame_transposed = torch.transpose(context_frame,1,2)\n",
    "            # CV = BS x HS\n",
    "            context_vector = torch.bmm(context_frame_transposed,attention_score).squeeze(2)\n",
    "            # CH = BS x HS\n",
    "            current_hidden = current_hidden.squeeze(2)\n",
    "            # CC = BS x 1 x 2HS\n",
    "            # combined_context = torch.cat((current_hidden,context_vector),1).unsqueeze(1)\n",
    "            combined_contexts[:,t,:] = context_vector\n",
    "            \n",
    "        # CCs = BS x MS x 2HS\n",
    "        # O = BS x MS x V\n",
    "        x = self.output_layer(combined_contexts)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "What is the current gru4recFC outputting?\n",
    "How far in the future do we need to predict?\n",
    "Is our problem formulation compatible enough  w/ seq2seq?\n",
    "Max length vs. set length of input?\n",
    "How much to give encoder, how much to expect from decoder\n",
    "\n",
    "Resources:\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Mar 23 08:39:11 2021\n",
    "\n",
    "@author: lpott\n",
    "\"\"\"\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing import *\n",
    "from dataset import *\n",
    "from metrics import *\n",
    "from model import *\n",
    "from utils import bert2dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "\n",
    "read_filename =\"ml-1m\\\\ratings.dat\"\n",
    "read_bert_filename = \"bert_sequence_20m.txt\"\n",
    "read_movie_filename = \"\"#\"movies-1m.csv\"\n",
    "size = \"1m\"\n",
    "\n",
    "num_epochs = 100\n",
    "lr = 1e-2\n",
    "batch_size = 64\n",
    "reg = 1e-4\n",
    "train_method = \"normal\"\n",
    "\n",
    "\n",
    "hidden_dim = 256\n",
    "embedding_dim = 128\n",
    "bert_dim= 768\n",
    "window = 0\n",
    "\n",
    "freeze_plot = False\n",
    "tied = False\n",
    "dropout= 0\n",
    "\n",
    "k = 10\n",
    "max_length = 200\n",
    "min_len = 10\n",
    "\n",
    "\n",
    "# nextitnet options...\n",
    "hidden_layers = 3\n",
    "dilations = [1,2,2,4]\n",
    "\n",
    "model_type = \"attention\"\n",
    "# model_type = \"feature_add\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Creating DataFrame ==========\n",
      "user_id        6040\n",
      "item_id        3706\n",
      "rating            5\n",
      "timestamp    458455\n",
      "dtype: int64\n",
      "(1000209, 4)\n",
      "Minimum Session Length: 20\n",
      "Maximum Session Length: 2314\n",
      "Average Session Length: 165.60\n",
      "========== Filtering Sessions <= 10  DataFrame ==========\n",
      "user_id        6040\n",
      "item_id        3706\n",
      "rating            5\n",
      "timestamp    458455\n",
      "dtype: int64\n",
      "(1000209, 4)\n",
      "Minimum Session Length: 20\n",
      "Maximum Session Length: 2314\n",
      "Average Session Length: 165.60\n"
     ]
    }
   ],
   "source": [
    "# ------------------Data Initialization----------------------#\n",
    "\n",
    "# convert .dat file to time-sorted pandas dataframe\n",
    "ml_1m = create_df(read_filename,size=size)\n",
    "\n",
    "# remove users who have sessions lengths less than min_len\n",
    "ml_1m = filter_df(ml_1m,item_min=min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n"
     ]
    }
   ],
   "source": [
    "# ------------------Data Initialization----------------------#\n",
    "if read_movie_filename != \"\":\n",
    "    ml_movie_df = create_movie_df(read_movie_filename,size=size)\n",
    "    ml_movie_df = convert_genres(ml_movie_df)\n",
    "    \n",
    "    # initialize reset object\n",
    "    reset_object = reset_df()\n",
    "    \n",
    "    # map all user ids, item ids, and genres to range 0 - number of users/items/genres\n",
    "    ml_1m,ml_movie_df = reset_object.fit_transform(ml_1m,ml_movie_df)\n",
    "    \n",
    "    # value that padded genre tokens shall take\n",
    "    pad_genre_token = reset_object.genre_enc.transform([\"NULL\"]).item()\n",
    "    \n",
    "    genre_dim = len(np.unique(np.concatenate(ml_movie_df.genre))) - 1\n",
    "\n",
    "else:\n",
    "    # initialize reset object\n",
    "    reset_object = reset_df()\n",
    "    \n",
    "    # map all user ids and item ids to range 0 - Number of Users/Items \n",
    "    # i.e. [1,7,5] -> [0,2,1]\n",
    "    ml_1m = reset_object.fit_transform(ml_1m)\n",
    "    \n",
    "    pad_genre_token = None\n",
    "    ml_movie_df = None\n",
    "    genre_dim = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Reading .txt file with all item id and embeddings ==========\n"
     ]
    }
   ],
   "source": [
    "# ------------------Data Initialization----------------------#\n",
    "# how many unique users, items, ratings and timestamps are there\n",
    "n_users,n_items,n_ratings,n_timestamp = ml_1m.nunique()\n",
    "\n",
    "# value that padded tokens shall take\n",
    "pad_token = n_items\n",
    "\n",
    "# the output dimension for softmax layer\n",
    "output_dim = n_items\n",
    "\n",
    "\n",
    "# get the item id : bert plot embedding dictionary\n",
    "if bert_dim != 0:\n",
    "    feature_embed = bert2dict(bert_filename=read_bert_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                              | 57/6040 [00:00<00:10, 564.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Creating User Histories ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6040/6040 [00:10<00:00, 569.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of every user's session (history)\n",
    "# i.e. {user: [user clicks]}\n",
    "if size == \"1m\":\n",
    "    user_history = create_user_history(ml_1m)\n",
    "\n",
    "elif size == \"20m\":\n",
    "    import pickle\n",
    "    with open('userhistory.pickle', 'rb') as handle:\n",
    "        user_history = pickle.load(handle)\n",
    "# create a dictionary of all items a user has not clicked\n",
    "# i.e. {user: [items not clicked by user]}\n",
    "# user_noclicks = create_user_noclick(user_history,ml_1m,n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 6040/6040 [00:00<00:00, 147317.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Splitting User Histories into Train, Validation, and Test Splits ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# split data by leave-one-out strategy\n",
    "# have train dictionary {user: [last 41 items prior to last 2 items in user session]}\n",
    "# have val dictionary {user: [last 41 items prior to last item in user session]}\n",
    "# have test dictionary {user: [last 41 items]}\n",
    "# i.e. if max_length = 4, [1,2,3,4,5,6] -> [1,2,3,4] , [2,3,4,5] , [3,4,5,6]\n",
    "train_history,val_history,test_history = train_val_test_split(user_history,max_length=max_length)\n",
    "\n",
    "# initialize the train,validation, and test pytorch dataset objects\n",
    "# eval pads all items except last token to predict\n",
    "train_dataset = GRUDataset(train_history,genre_df=ml_movie_df,mode='train',max_length=max_length,pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
    "val_dataset = GRUDataset(val_history,genre_df=ml_movie_df,mode='eval',max_length=max_length,pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
    "test_dataset = GRUDataset(test_history,genre_df=ml_movie_df,mode='eval',max_length=max_length,pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
    "\n",
    "# create the train,validation, and test pytorch dataloader objects\n",
    "train_dl = DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
    "val_dl = DataLoader(val_dataset,batch_size=64)\n",
    "test_dl = DataLoader(test_dataset,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert dim: 768\n",
      "Genre dim: 0\n",
      "Pad Token: 3706\n",
      "Pad Genre Token: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Bert dim: {:d}\".format(bert_dim))\n",
    "print(\"Genre dim: {:d}\".format(genre_dim))\n",
    "print(\"Pad Token: {}\".format(pad_token))\n",
    "print(\"Pad Genre Token: {}\".format(pad_genre_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------Model Initialization----------------------#\n",
    "\n",
    "# initialize gru4rec model with arguments specified earlier\n",
    "if model_type == \"feature_add\":\n",
    "    model = gru4recF(embedding_dim=embedding_dim,\n",
    "             hidden_dim=hidden_dim,\n",
    "             output_dim=output_dim,\n",
    "             genre_dim=genre_dim,\n",
    "             batch_first=True,\n",
    "             max_length=max_length,\n",
    "             pad_token=pad_token,\n",
    "             pad_genre_token=pad_genre_token,\n",
    "             bert_dim=bert_dim,\n",
    "             tied = tied,\n",
    "             dropout=dropout)\n",
    "\n",
    "\n",
    "if model_type == \"feature_concat\":\n",
    "    model = gru4recFC(embedding_dim=embedding_dim,\n",
    "             hidden_dim=hidden_dim,\n",
    "             output_dim=output_dim,\n",
    "             genre_dim=genre_dim,\n",
    "             batch_first=True,\n",
    "             max_length=max_length,\n",
    "             pad_token=pad_token,\n",
    "             pad_genre_token=pad_genre_token,\n",
    "             bert_dim=bert_dim,\n",
    "             tied = tied,\n",
    "             dropout=dropout)\n",
    "\n",
    "if model_type == \"vanilla\":\n",
    "    model = gru4rec_vanilla(hidden_dim=hidden_dim,\n",
    "                            output_dim=output_dim,\n",
    "                            batch_first=True,\n",
    "                            max_length=max_length,\n",
    "                            pad_token=pad_token,\n",
    "                            tied=tied,\n",
    "                            embedding_dim=embedding_dim)\n",
    "\n",
    "if model_type ==\"feature_only\":\n",
    "    model = gru4rec_feature(hidden_dim=hidden_dim,\n",
    "                            output_dim=output_dim,\n",
    "                            batch_first=True,\n",
    "                            max_length=max_length,\n",
    "                            pad_token=pad_token,\n",
    "                            bert_dim=bert_dim)\n",
    "\n",
    "if model_type == \"conv\":\n",
    "    model = gru4rec_conv(embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 batch_first=True,\n",
    "                 max_length=200,\n",
    "                 pad_token=0,\n",
    "                 dropout=0,\n",
    "                 window=3,\n",
    "                 tied=tied)\n",
    "    \n",
    "if model_type == \"nextitnet\":\n",
    "    model = NextItNet(embedding_dim=embedding_dim,\n",
    "                      output_dim=output_dim,\n",
    "                      hidden_layers=hidden_layers,\n",
    "                      dilations=dilations,\n",
    "                      pad_token=n_items,\n",
    "                      max_len=max_length)\n",
    "\n",
    "if model_type == \"attention\":\n",
    "    model = gru4recF(embedding_dim=embedding_dim,\n",
    "             hidden_dim=hidden_dim,\n",
    "             output_dim=output_dim,\n",
    "             genre_dim=genre_dim,\n",
    "             batch_first=True,\n",
    "             max_length=max_length,\n",
    "             pad_token=pad_token,\n",
    "             pad_genre_token=pad_genre_token,\n",
    "             bert_dim=bert_dim,\n",
    "             tied = tied,\n",
    "             dropout=dropout)\n",
    "#     modelD = gru4recF_decoder(hidden_dim=hidden_dim, output_dim=output_dim, dropout=0, max_length=max_length)\n",
    "#     modelD = modelD.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bert_dim != 0:\n",
    "    model.init_weight(reset_object,feature_embed)\n",
    "    \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot_embedding.weight',\n",
       " 'plot_projection.weight',\n",
       " 'plot_projection.bias',\n",
       " 'encoder_layer.weight_ih_l0',\n",
       " 'encoder_layer.weight_hh_l0',\n",
       " 'encoder_layer.bias_ih_l0',\n",
       " 'encoder_layer.bias_hh_l0',\n",
       " 'output_layer.weight',\n",
       " 'output_layer.bias']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name,param in model.named_parameters() if ((\"movie\" not in name) or (\"plot_embedding\" in name) or (\"genre\" in name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie_embedding.weight',\n",
       " 'encoder_layer.weight_ih_l0',\n",
       " 'encoder_layer.weight_hh_l0',\n",
       " 'encoder_layer.bias_ih_l0',\n",
       " 'encoder_layer.bias_hh_l0',\n",
       " 'output_layer.weight',\n",
       " 'output_layer.bias']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name,param in model.named_parameters() if (\"plot\" not in name) and (\"genre\" not in name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Adam optimizer with gru4rec model parameters\n",
    "if train_method != \"normal\":\n",
    "    optimizer_features = torch.optim.Adam([param for name,param in model.named_parameters() if ((\"movie\" not in name) or (\"plot_embedding\" in name) or (\"genre\" in name)) ],\n",
    "                                          lr=lr/10,weight_decay=reg)\n",
    "    \n",
    "    optimizer_ids = torch.optim.Adam([param for name,param in model.named_parameters() if (\"plot\" not in name) and (\"genre\" not in name)],\n",
    "                                     lr=lr,weight_decay=reg)\n",
    "\n",
    "elif train_method == \"normal\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=reg)\n",
    "    decoder_optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=reg)\n",
    "if freeze_plot and bert_dim !=0:\n",
    "    model.plot_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=n_items)\n",
    "#Recall_Object = Recall_E_prob(ml_1m,user_history,n_users,n_items,k=k)\n",
    "#Recall_Object = Recall_E_Noprob(ml_1m,user_history,n_users,n_items,k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Creating Hit@10 Metric Object ==========\n"
     ]
    }
   ],
   "source": [
    "Recall_Object = Recall_E_prob(ml_1m,user_history,n_users,n_items,k=k,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch 1 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.28it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 7.39208\n",
      "Training Hits@10: 34.64\n",
      "Validation Hits@10: 32.25\n",
      "Testing Hits@10: 30.15\n",
      "==================== Epoch 2 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.36it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 6.46878\n",
      "Training Hits@10: 59.64\n",
      "Validation Hits@10: 55.41\n",
      "Testing Hits@10: 52.50\n",
      "==================== Epoch 3 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.32it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 6.04423\n",
      "Training Hits@10: 66.04\n",
      "Validation Hits@10: 62.48\n",
      "Testing Hits@10: 58.15\n",
      "==================== Epoch 4 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.43it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.88014\n",
      "Training Hits@10: 68.34\n",
      "Validation Hits@10: 64.39\n",
      "Testing Hits@10: 60.33\n",
      "==================== Epoch 5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.37it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.79427\n",
      "Training Hits@10: 70.66\n",
      "Validation Hits@10: 66.85\n",
      "Testing Hits@10: 61.72\n",
      "==================== Epoch 6 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.42it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.73500\n",
      "Training Hits@10: 70.81\n",
      "Validation Hits@10: 66.95\n",
      "Testing Hits@10: 61.97\n",
      "==================== Epoch 7 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.50it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.69409\n",
      "Training Hits@10: 72.24\n",
      "Validation Hits@10: 67.57\n",
      "Testing Hits@10: 62.40\n",
      "==================== Epoch 8 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.24it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.68849\n",
      "Training Hits@10: 72.20\n",
      "Validation Hits@10: 67.68\n",
      "Testing Hits@10: 63.49\n",
      "==================== Epoch 9 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.35it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.64308\n",
      "Training Hits@10: 73.25\n",
      "Validation Hits@10: 68.36\n",
      "Testing Hits@10: 65.02\n",
      "==================== Epoch 10 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.25it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.62958\n",
      "Training Hits@10: 73.20\n",
      "Validation Hits@10: 68.66\n",
      "Testing Hits@10: 64.39\n",
      "==================== Epoch 11 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.43it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.61060\n",
      "Training Hits@10: 72.63\n",
      "Validation Hits@10: 68.58\n",
      "Testing Hits@10: 63.66\n",
      "==================== Epoch 12 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.37it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.60677\n",
      "Training Hits@10: 73.56\n",
      "Validation Hits@10: 68.49\n",
      "Testing Hits@10: 64.65\n",
      "==================== Epoch 13 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.56it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.58658\n",
      "Training Hits@10: 74.07\n",
      "Validation Hits@10: 68.79\n",
      "Testing Hits@10: 64.82\n",
      "==================== Epoch 14 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.54it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.60954\n",
      "Training Hits@10: 74.19\n",
      "Validation Hits@10: 69.57\n",
      "Testing Hits@10: 65.07\n",
      "==================== Epoch 15 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.49it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.58192\n",
      "Training Hits@10: 73.61\n",
      "Validation Hits@10: 68.66\n",
      "Testing Hits@10: 64.82\n",
      "==================== Epoch 16 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.60it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.57142\n",
      "Training Hits@10: 73.66\n",
      "Validation Hits@10: 69.16\n",
      "Testing Hits@10: 64.70\n",
      "==================== Epoch 17 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.55it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.57898\n",
      "Training Hits@10: 74.29\n",
      "Validation Hits@10: 69.17\n",
      "Testing Hits@10: 64.97\n",
      "==================== Epoch 18 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.58it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.56743\n",
      "Training Hits@10: 75.13\n",
      "Validation Hits@10: 69.64\n",
      "Testing Hits@10: 65.26\n",
      "==================== Epoch 19 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.58it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.55689\n",
      "Training Hits@10: 74.93\n",
      "Validation Hits@10: 69.62\n",
      "Testing Hits@10: 64.98\n",
      "==================== Epoch 20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.58it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.55075\n",
      "Training Hits@10: 74.65\n",
      "Validation Hits@10: 69.29\n",
      "Testing Hits@10: 65.23\n",
      "==================== Epoch 21 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.60it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.54799\n",
      "Training Hits@10: 74.65\n",
      "Validation Hits@10: 69.40\n",
      "Testing Hits@10: 65.18\n",
      "==================== Epoch 22 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.39it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.55011\n",
      "Training Hits@10: 73.66\n",
      "Validation Hits@10: 69.45\n",
      "Testing Hits@10: 65.00\n",
      "==================== Epoch 23 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.56it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.55809\n",
      "Training Hits@10: 75.00\n",
      "Validation Hits@10: 68.82\n",
      "Testing Hits@10: 65.40\n",
      "==================== Epoch 24 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.26it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.54755\n",
      "Training Hits@10: 75.55\n",
      "Validation Hits@10: 70.35\n",
      "Testing Hits@10: 66.08\n",
      "==================== Epoch 25 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.29it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.54243\n",
      "Training Hits@10: 74.98\n",
      "Validation Hits@10: 69.67\n",
      "Testing Hits@10: 66.56\n",
      "==================== Epoch 26 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.23it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.53240\n",
      "Training Hits@10: 75.20\n",
      "Validation Hits@10: 69.50\n",
      "Testing Hits@10: 64.93\n",
      "==================== Epoch 27 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.24it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.54912\n",
      "Training Hits@10: 74.87\n",
      "Validation Hits@10: 69.80\n",
      "Testing Hits@10: 64.92\n",
      "==================== Epoch 28 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  5.95it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.53671\n",
      "Training Hits@10: 75.26\n",
      "Validation Hits@10: 69.45\n",
      "Testing Hits@10: 65.58\n",
      "==================== Epoch 29 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.23it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.53026\n",
      "Training Hits@10: 75.05\n",
      "Validation Hits@10: 70.17\n",
      "Testing Hits@10: 66.24\n",
      "==================== Epoch 30 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.20it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.52495\n",
      "Training Hits@10: 74.64\n",
      "Validation Hits@10: 68.94\n",
      "Testing Hits@10: 65.02\n",
      "==================== Epoch 31 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.33it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.53050\n",
      "Training Hits@10: 75.45\n",
      "Validation Hits@10: 69.60\n",
      "Testing Hits@10: 65.46\n",
      "==================== Epoch 32 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.26it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.52330\n",
      "Training Hits@10: 75.65\n",
      "Validation Hits@10: 69.65\n",
      "Testing Hits@10: 65.81\n",
      "==================== Epoch 33 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.66it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.52111\n",
      "Training Hits@10: 75.68\n",
      "Validation Hits@10: 70.38\n",
      "Testing Hits@10: 66.08\n",
      "==================== Epoch 34 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.84it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:15,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.52108\n",
      "Training Hits@10: 75.03\n",
      "Validation Hits@10: 69.85\n",
      "Testing Hits@10: 65.94\n",
      "==================== Epoch 35 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.10it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.56840\n",
      "Training Hits@10: 75.55\n",
      "Validation Hits@10: 70.02\n",
      "Testing Hits@10: 65.89\n",
      "==================== Epoch 36 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.97it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.52141\n",
      "Training Hits@10: 75.68\n",
      "Validation Hits@10: 69.92\n",
      "Testing Hits@10: 65.71\n",
      "==================== Epoch 37 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.86it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.52395\n",
      "Training Hits@10: 75.07\n",
      "Validation Hits@10: 69.27\n",
      "Testing Hits@10: 65.48\n",
      "==================== Epoch 38 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.83it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.51950\n",
      "Training Hits@10: 75.41\n",
      "Validation Hits@10: 69.44\n",
      "Testing Hits@10: 65.55\n",
      "==================== Epoch 39 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.82it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.51309\n",
      "Training Hits@10: 74.92\n",
      "Validation Hits@10: 69.75\n",
      "Testing Hits@10: 65.46\n",
      "==================== Epoch 40 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.79it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.51056\n",
      "Training Hits@10: 76.11\n",
      "Validation Hits@10: 69.95\n",
      "Testing Hits@10: 65.83\n",
      "==================== Epoch 41 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.82it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.51380\n",
      "Training Hits@10: 75.75\n",
      "Validation Hits@10: 70.07\n",
      "Testing Hits@10: 65.23\n",
      "==================== Epoch 42 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.86it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.51063\n",
      "Training Hits@10: 76.24\n",
      "Validation Hits@10: 70.08\n",
      "Testing Hits@10: 66.01\n",
      "==================== Epoch 43 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.88it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.50113\n",
      "Training Hits@10: 75.33\n",
      "Validation Hits@10: 70.33\n",
      "Testing Hits@10: 65.96\n",
      "==================== Epoch 44 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.20it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:15,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.50498\n",
      "Training Hits@10: 74.88\n",
      "Validation Hits@10: 69.83\n",
      "Testing Hits@10: 65.53\n",
      "==================== Epoch 45 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.19it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.50315\n",
      "Training Hits@10: 75.75\n",
      "Validation Hits@10: 69.93\n",
      "Testing Hits@10: 65.07\n",
      "==================== Epoch 46 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.97it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.50567\n",
      "Training Hits@10: 75.46\n",
      "Validation Hits@10: 69.62\n",
      "Testing Hits@10: 65.33\n",
      "==================== Epoch 47 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.89it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.51121\n",
      "Training Hits@10: 75.56\n",
      "Validation Hits@10: 70.08\n",
      "Testing Hits@10: 66.26\n",
      "==================== Epoch 48 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.56it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.50526\n",
      "Training Hits@10: 76.13\n",
      "Validation Hits@10: 70.07\n",
      "Testing Hits@10: 66.14\n",
      "==================== Epoch 49 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.52it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.49937\n",
      "Training Hits@10: 75.13\n",
      "Validation Hits@10: 69.35\n",
      "Testing Hits@10: 65.40\n",
      "==================== Epoch 50 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.56it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.50510\n",
      "Training Hits@10: 76.09\n",
      "Validation Hits@10: 69.39\n",
      "Testing Hits@10: 65.83\n",
      "==================== Epoch 51 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.54it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.50155\n",
      "Training Hits@10: 74.97\n",
      "Validation Hits@10: 70.36\n",
      "Testing Hits@10: 66.03\n",
      "==================== Epoch 52 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.53it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.49650\n",
      "Training Hits@10: 76.37\n",
      "Validation Hits@10: 69.87\n",
      "Testing Hits@10: 66.46\n",
      "==================== Epoch 53 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.51it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.49661\n",
      "Training Hits@10: 75.96\n",
      "Validation Hits@10: 69.02\n",
      "Testing Hits@10: 65.30\n",
      "==================== Epoch 54 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.49it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.49907\n",
      "Training Hits@10: 75.45\n",
      "Validation Hits@10: 70.13\n",
      "Testing Hits@10: 66.46\n",
      "==================== Epoch 55 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.54it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.58630\n",
      "Training Hits@10: 74.98\n",
      "Validation Hits@10: 69.67\n",
      "Testing Hits@10: 65.60\n",
      "==================== Epoch 56 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.56it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.53040\n",
      "Training Hits@10: 75.81\n",
      "Validation Hits@10: 69.67\n",
      "Testing Hits@10: 65.41\n",
      "==================== Epoch 57 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.51it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.51143\n",
      "Training Hits@10: 76.08\n",
      "Validation Hits@10: 71.03\n",
      "Testing Hits@10: 65.53\n",
      "==================== Epoch 58 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.54it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.49596\n",
      "Training Hits@10: 75.83\n",
      "Validation Hits@10: 70.10\n",
      "Testing Hits@10: 65.75\n",
      "==================== Epoch 59 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:15<00:00,  6.20it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.49405\n",
      "Training Hits@10: 76.39\n",
      "Validation Hits@10: 70.75\n",
      "Testing Hits@10: 66.67\n",
      "==================== Epoch 60 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.53it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.49625\n",
      "Training Hits@10: 76.32\n",
      "Validation Hits@10: 70.43\n",
      "Testing Hits@10: 66.41\n",
      "==================== Epoch 61 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:17<00:00,  5.34it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48792\n",
      "Training Hits@10: 76.57\n",
      "Validation Hits@10: 70.46\n",
      "Testing Hits@10: 65.71\n",
      "==================== Epoch 62 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.85it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48465\n",
      "Training Hits@10: 76.08\n",
      "Validation Hits@10: 69.98\n",
      "Testing Hits@10: 66.18\n",
      "==================== Epoch 63 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.77it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48627\n",
      "Training Hits@10: 75.99\n",
      "Validation Hits@10: 70.17\n",
      "Testing Hits@10: 66.11\n",
      "==================== Epoch 64 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.15it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48797\n",
      "Training Hits@10: 76.42\n",
      "Validation Hits@10: 70.20\n",
      "Testing Hits@10: 65.93\n",
      "==================== Epoch 65 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.86it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48814\n",
      "Training Hits@10: 76.27\n",
      "Validation Hits@10: 69.97\n",
      "Testing Hits@10: 66.49\n",
      "==================== Epoch 66 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.17it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48587\n",
      "Training Hits@10: 75.35\n",
      "Validation Hits@10: 70.08\n",
      "Testing Hits@10: 65.55\n",
      "==================== Epoch 67 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.15it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48347\n",
      "Training Hits@10: 76.39\n",
      "Validation Hits@10: 69.64\n",
      "Testing Hits@10: 66.13\n",
      "==================== Epoch 68 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.06it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48615\n",
      "Training Hits@10: 75.78\n",
      "Validation Hits@10: 70.31\n",
      "Testing Hits@10: 66.34\n",
      "==================== Epoch 69 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.84it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48812\n",
      "Training Hits@10: 76.03\n",
      "Validation Hits@10: 69.83\n",
      "Testing Hits@10: 66.08\n",
      "==================== Epoch 70 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.57it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48153\n",
      "Training Hits@10: 76.39\n",
      "Validation Hits@10: 70.05\n",
      "Testing Hits@10: 65.58\n",
      "==================== Epoch 71 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.81it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48709\n",
      "Training Hits@10: 76.67\n",
      "Validation Hits@10: 69.83\n",
      "Testing Hits@10: 66.13\n",
      "==================== Epoch 72 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.80it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48052\n",
      "Training Hits@10: 75.40\n",
      "Validation Hits@10: 69.64\n",
      "Testing Hits@10: 65.91\n",
      "==================== Epoch 73 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.76it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48918\n",
      "Training Hits@10: 76.04\n",
      "Validation Hits@10: 70.18\n",
      "Testing Hits@10: 66.44\n",
      "==================== Epoch 74 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.80it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48417\n",
      "Training Hits@10: 77.00\n",
      "Validation Hits@10: 70.75\n",
      "Testing Hits@10: 66.89\n",
      "==================== Epoch 75 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.77it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48203\n",
      "Training Hits@10: 75.84\n",
      "Validation Hits@10: 70.40\n",
      "Testing Hits@10: 65.98\n",
      "==================== Epoch 76 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.84it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48471\n",
      "Training Hits@10: 76.13\n",
      "Validation Hits@10: 70.35\n",
      "Testing Hits@10: 66.57\n",
      "==================== Epoch 77 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.87it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47922\n",
      "Training Hits@10: 75.81\n",
      "Validation Hits@10: 69.90\n",
      "Testing Hits@10: 65.73\n",
      "==================== Epoch 78 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.80it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48569\n",
      "Training Hits@10: 76.14\n",
      "Validation Hits@10: 70.05\n",
      "Testing Hits@10: 66.29\n",
      "==================== Epoch 79 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:14<00:00,  6.73it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:17,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47859\n",
      "Training Hits@10: 75.94\n",
      "Validation Hits@10: 70.05\n",
      "Testing Hits@10: 66.46\n",
      "==================== Epoch 80 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  6.99it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48490\n",
      "Training Hits@10: 76.16\n",
      "Validation Hits@10: 70.00\n",
      "Testing Hits@10: 66.09\n",
      "==================== Epoch 81 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.21it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48052\n",
      "Training Hits@10: 76.08\n",
      "Validation Hits@10: 69.55\n",
      "Testing Hits@10: 65.79\n",
      "==================== Epoch 82 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.18it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47571\n",
      "Training Hits@10: 75.53\n",
      "Validation Hits@10: 70.26\n",
      "Testing Hits@10: 65.75\n",
      "==================== Epoch 83 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.19it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47641\n",
      "Training Hits@10: 76.23\n",
      "Validation Hits@10: 70.08\n",
      "Testing Hits@10: 66.44\n",
      "==================== Epoch 84 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.18it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47502\n",
      "Training Hits@10: 75.75\n",
      "Validation Hits@10: 69.72\n",
      "Testing Hits@10: 66.18\n",
      "==================== Epoch 85 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.10it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47840\n",
      "Training Hits@10: 75.84\n",
      "Validation Hits@10: 69.83\n",
      "Testing Hits@10: 65.66\n",
      "==================== Epoch 86 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.11it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47968\n",
      "Training Hits@10: 76.44\n",
      "Validation Hits@10: 70.08\n",
      "Testing Hits@10: 65.79\n",
      "==================== Epoch 87 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.14it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:15,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48088\n",
      "Training Hits@10: 75.10\n",
      "Validation Hits@10: 68.77\n",
      "Testing Hits@10: 65.08\n",
      "==================== Epoch 88 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.12it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48868\n",
      "Training Hits@10: 76.54\n",
      "Validation Hits@10: 70.30\n",
      "Testing Hits@10: 66.23\n",
      "==================== Epoch 89 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.13it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:15,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47456\n",
      "Training Hits@10: 76.26\n",
      "Validation Hits@10: 69.92\n",
      "Testing Hits@10: 65.99\n",
      "==================== Epoch 90 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.07it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.51109\n",
      "Training Hits@10: 76.75\n",
      "Validation Hits@10: 70.00\n",
      "Testing Hits@10: 65.58\n",
      "==================== Epoch 91 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.15it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.48741\n",
      "Training Hits@10: 76.52\n",
      "Validation Hits@10: 70.50\n",
      "Testing Hits@10: 66.27\n",
      "==================== Epoch 92 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.14it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:15,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47361\n",
      "Training Hits@10: 76.59\n",
      "Validation Hits@10: 70.13\n",
      "Testing Hits@10: 67.00\n",
      "==================== Epoch 93 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.16it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47312\n",
      "Training Hits@10: 76.08\n",
      "Validation Hits@10: 70.28\n",
      "Testing Hits@10: 66.39\n",
      "==================== Epoch 94 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.15it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47848\n",
      "Training Hits@10: 76.31\n",
      "Validation Hits@10: 70.46\n",
      "Testing Hits@10: 66.13\n",
      "==================== Epoch 95 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.12it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:15,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47321\n",
      "Training Hits@10: 76.03\n",
      "Validation Hits@10: 69.67\n",
      "Testing Hits@10: 65.86\n",
      "==================== Epoch 96 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.11it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47571\n",
      "Training Hits@10: 76.85\n",
      "Validation Hits@10: 69.69\n",
      "Testing Hits@10: 66.21\n",
      "==================== Epoch 97 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.16it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:15,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.46896\n",
      "Training Hits@10: 76.23\n",
      "Validation Hits@10: 69.12\n",
      "Testing Hits@10: 65.43\n",
      "==================== Epoch 98 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.20it/s]\n",
      "  1%|▊                                                                                  | 1/95 [00:00<00:16,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.46607\n",
      "Training Hits@10: 76.64\n",
      "Validation Hits@10: 70.00\n",
      "Testing Hits@10: 66.41\n",
      "==================== Epoch 99 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.16it/s]\n",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47178\n",
      "Training Hits@10: 75.79\n",
      "Validation Hits@10: 70.63\n",
      "Testing Hits@10: 66.61\n",
      "==================== Epoch 100 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:13<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CE Loss: 5.47857\n",
      "Training Hits@10: 76.85\n",
      "Validation Hits@10: 70.56\n",
      "Testing Hits@10: 66.24\n",
      "====================================================================================================\n",
      "Maximum Training Hit@10: 76.08\n",
      "Maximum Validation Hit@10: 71.03\n",
      "Maximum Testing Hit@10: 65.53\n"
     ]
    }
   ],
   "source": [
    "# ------------------Training Initialization----------------------#\n",
    "max_train_hit = 0\n",
    "max_val_hit = 0\n",
    "max_test_hit = 0\n",
    "\n",
    "i = 0;\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"=\"*20,\"Epoch {}\".format(epoch+1),\"=\"*20)\n",
    "    \n",
    "    model.train()  \n",
    "    \n",
    "    running_loss = 0\n",
    "\n",
    "    for j,data in enumerate(tqdm(train_dl,position=0,leave=True)):\n",
    "        \n",
    "        if train_method != \"normal\":\n",
    "            optimizer_features.zero_grad()\n",
    "            optimizer_ids.zero_grad()\n",
    "            \n",
    "        elif train_method == \"normal\": \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        if genre_dim != 0:            \n",
    "            inputs,genre_inputs,labels,x_lens,uid = data\n",
    "            outputs = model(x=inputs.to(device),x_lens=x_lens.squeeze().tolist(),x_genre=genre_inputs.to(device))\n",
    "        \n",
    "        elif genre_dim == 0:\n",
    "            inputs,labels,x_lens,uid = data \n",
    "            outputs = model(x=inputs.to(device),x_lens=x_lens.squeeze().tolist())\n",
    "       \n",
    "        if tied:\n",
    "            outputs_ignore_pad = outputs[:,:,:-1]\n",
    "            loss = loss_fn(outputs_ignore_pad.view(-1,outputs_ignore_pad.size(-1)),labels.view(-1).to(device))\n",
    "            \n",
    "        else:\n",
    "            loss = loss_fn(outputs.view(-1,outputs.size(-1)),labels.view(-1).to(device))\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        if train_method != \"normal\":\n",
    "            if train_method == \"interleave\":\n",
    "                # interleave on the epochs\n",
    "                if (j+1) % 2 == 0:\n",
    "                    optimizer_features.step()\n",
    "                else:\n",
    "                    optimizer_ids.step()\n",
    "\n",
    "            elif train_method == \"alternate\":\n",
    "                if (epoch+1) % 2 == 0:\n",
    "                    optimizer_features.step()\n",
    "                else:\n",
    "                    optimizer_ids.step()\n",
    "        \n",
    "    \n",
    "                    \n",
    "        elif train_method == \"normal\":\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.detach().cpu().item()\n",
    "\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    training_hit = Recall_Object(model,train_dl,\"train\")\n",
    "    validation_hit = Recall_Object(model,val_dl,\"validation\")\n",
    "    testing_hit = Recall_Object(model,test_dl,\"test\")\n",
    "    \n",
    "    if max_val_hit < validation_hit:\n",
    "        max_val_hit = validation_hit\n",
    "        max_test_hit = testing_hit\n",
    "        max_train_hit = training_hit\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training CE Loss: {:.5f}\".format(running_loss/len(train_dl)))\n",
    "    print(\"Training Hits@{:d}: {:.2f}\".format(k,training_hit))\n",
    "    print(\"Validation Hits@{:d}: {:.2f}\".format(k,validation_hit))\n",
    "    print(\"Testing Hits@{:d}: {:.2f}\".format(k,testing_hit))\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Maximum Training Hit@{:d}: {:.2f}\".format(k,max_train_hit))\n",
    "print(\"Maximum Validation Hit@{:d}: {:.2f}\".format(k,max_val_hit))\n",
    "print(\"Maximum Testing Hit@{:d}: {:.2f}\".format(k,max_test_hit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch 1 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [02:24<00:00,  1.52s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4f3a3a3c9e40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[0mtraining_hit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRecall_Object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[0mvalidation_hit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRecall_Object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"validation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mtesting_hit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRecall_Object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\CS 247\\PROJECT\\attentive-session-based-recs\\metrics.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, model, dataloader, mode)\u001b[0m\n\u001b[0;32m     70\u001b[0m                     \u001b[0msample_negatives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_lens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                     \u001b[0mtopk_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_lens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_negatives\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdescending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m                     \u001b[0mtotal_hits\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopk_items\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# ------------------Training Initialization----------------------#\n",
    "max_train_hit = 0\n",
    "max_val_hit = 0\n",
    "max_test_hit = 0\n",
    "\n",
    "#device = 'cpu'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"=\"*20,\"Epoch {}\".format(epoch+1),\"=\"*20)\n",
    "    \n",
    "    model.train()  \n",
    "    \n",
    "    running_loss = 0\n",
    "\n",
    "    for j,data in enumerate(tqdm(train_dl,position=0,leave=True)):\n",
    "        \n",
    "        if train_method != \"normal\":\n",
    "            optimizer_features.zero_grad()\n",
    "            optimizer_ids.zero_grad()\n",
    "            \n",
    "        elif train_method == \"normal\": \n",
    "            optimizer.zero_grad()\n",
    "            if model_type == \"attention\":\n",
    "                decoder_optimizer.zero_grad()\n",
    "        \n",
    "        if genre_dim != 0:            \n",
    "            inputs,genre_inputs,labels,x_lens,uid = data\n",
    "            outputs = model(x=inputs.cuda(),x_lens=x_lens.squeeze().tolist(),x_genre=genre_inputs.cuda())\n",
    "        \n",
    "        elif genre_dim == 0:\n",
    "            # input = B * max seq\n",
    "            # labels = B * max seq\n",
    "            # x_lens = B, length of each input\n",
    "            \n",
    "            inputs,labels,x_lens,uid = data\n",
    "            # pseudo code\n",
    "            # iterate thru inputs to generate encoder outputs (first for loop)\n",
    "            # use encoder outputs to create initial hidden context vector, use SOS token to create initial decoder input\n",
    "            # use second for loop to generate M number of outputs, then save those to the outputs variable\n",
    "            # questions:\n",
    "            # how should I iterate thru the given data set\n",
    "            # how many outputs should I use?\n",
    "            # format of input / output is useful\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            encoder_outputs, hidden_states = model(x=inputs.to(device),x_lens=x_lens.squeeze().tolist())\n",
    "            decoder_inputs = inputs[:,0].view(1,-1).to(device)\n",
    "            decoder_hidden = hidden_states\n",
    "            \n",
    "            outputs = torch.zeros(inputs.size()[0],max_length,output_dim, device=device)\n",
    "            \n",
    "            for i in range(max_length):\n",
    "#                 print(\"Step {}\".format(i))\n",
    "#                 print(\"Decoder Input Dimension: \")\n",
    "#                 print(decoder_inputs.size())\n",
    "#                 print(\"Decoder Hidden States Dimension: \")\n",
    "#                 print(decoder_hidden.size())\n",
    "                decoder_outputs, decoder_hidden, decoder_attention = modelD(decoder_inputs, decoder_hidden, encoder_outputs)\n",
    "                outputs[:,i,:] = decoder_outputs\n",
    "                decoder_inputs = labels[:,i].view(1,-1)\n",
    "#                 print(decoder_inputs)\n",
    "    \n",
    "        \n",
    "            \n",
    "       \n",
    "        if tied:\n",
    "            outputs_ignore_pad = outputs[:,:,:-1]\n",
    "            loss = loss_fn(outputs_ignore_pad.view(-1,outputs_ignore_pad.size(-1)),labels.view(-1).cuda())\n",
    "            \n",
    "        else:\n",
    "            loss = loss_fn(outputs.view(-1,outputs.size(-1)),labels.view(-1).cuda())\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        if train_method != \"normal\":\n",
    "            if train_method == \"interleave\":\n",
    "                # interleave on the epochs\n",
    "                if (j+1) % 2 == 0:\n",
    "                    optimizer_features.step()\n",
    "                else:\n",
    "                    optimizer_ids.step()\n",
    "\n",
    "            elif train_method == \"alternate\":\n",
    "                if (epoch+1) % 2 == 0:\n",
    "                    optimizer_features.step()\n",
    "                else:\n",
    "                    optimizer_ids.step()\n",
    "\n",
    "        elif train_method == \"normal\":\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.detach().cpu().item()\n",
    "\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    training_hit = Recall_Object(model,train_dl,\"train\")\n",
    "    validation_hit = Recall_Object(model,val_dl,\"validation\")\n",
    "    testing_hit = Recall_Object(model,test_dl,\"test\")\n",
    "    \n",
    "    if max_val_hit < validation_hit:\n",
    "        max_val_hit = validation_hit\n",
    "        max_test_hit = testing_hit\n",
    "        max_train_hit = training_hit\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training CE Loss: {:.5f}\".format(running_loss/len(train_dl)))\n",
    "    print(\"Training Hits@{:d}: {:.2f}\".format(k,training_hit))\n",
    "    print(\"Validation Hits@{:d}: {:.2f}\".format(k,validation_hit))\n",
    "    print(\"Testing Hits@{:d}: {:.2f}\".format(k,testing_hit))\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Maximum Training Hit@{:d}: {:.2f}\".format(k,max_train_hit))\n",
    "print(\"Maximum Validation Hit@{:d}: {:.2f}\".format(k,max_val_hit))\n",
    "print(\"Maximum Testing Hit@{:d}: {:.2f}\".format(k,max_test_hit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Maximum Training Hit@{:d}: {:.2f}\".format(k,max_train_hit))\n",
    "print(\"Maximum Validation Hit@{:d}: {:.2f}\".format(k,max_val_hit))\n",
    "print(\"Maximum Testing Hit@{:d}: {:.2f}\".format(k,max_test_hit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[3, 10, 11, 13]\n"
     ]
    }
   ],
   "source": [
    "# input and label\n",
    "x = [5,3,10,11]\n",
    "y = [3,10,11,13]\n",
    "print(torch.cuda.is_available())\n",
    "print(y[0:5])\n",
    "# notes: gru layer stores hidden layer when using sequence input\n",
    "# use final hidden state from packed output\n",
    "# cross entropy loss used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
