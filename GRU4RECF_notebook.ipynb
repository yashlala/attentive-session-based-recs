{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Mar 23 08:39:11 2021\n",
    "\n",
    "@author: lpott\n",
    "\"\"\"\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing import *\n",
    "from dataset import *\n",
    "from metrics import *\n",
    "from model import *\n",
    "from utils import bert2dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "\n",
    "read_filename =\"ml-1m\\\\ratings.dat\"\n",
    "read_bert_filename = \"bert_sequence_20m.txt\"\n",
    "read_movie_filename = \"\"#\"movies-1m.csv\"\n",
    "size = \"1m\"\n",
    "\n",
    "num_epochs = 100\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "reg = 1e-5\n",
    "train_method = \"normal\"\n",
    "\n",
    "\n",
    "hidden_dim = 0\n",
    "embedding_dim = 64\n",
    "bert_dim= 0\n",
    "window = 0\n",
    "\n",
    "freeze_plot = False\n",
    "tied = False\n",
    "dropout= 0\n",
    "\n",
    "k = 10\n",
    "max_length = 200\n",
    "min_len = 10\n",
    "\n",
    "\n",
    "# nextitnet options...\n",
    "hidden_layers = 3\n",
    "dilations = [1,2,2,4]\n",
    "\n",
    "model_type = \"nextitnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Creating DataFrame ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpott\\Desktop\\UCLA\\COMSCI247-80\\attentive-session-based-recs\\preprocessing.py:32: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename,sep='::',header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id        6040\n",
      "item_id        3706\n",
      "rating            5\n",
      "timestamp    458455\n",
      "dtype: int64\n",
      "(1000209, 4)\n",
      "Minimum Session Length: 20\n",
      "Maximum Session Length: 2314\n",
      "Average Session Length: 165.60\n",
      "========== Filtering Sessions <= 10  DataFrame ==========\n",
      "user_id        6040\n",
      "item_id        3706\n",
      "rating            5\n",
      "timestamp    458455\n",
      "dtype: int64\n",
      "(1000209, 4)\n",
      "Minimum Session Length: 20\n",
      "Maximum Session Length: 2314\n",
      "Average Session Length: 165.60\n"
     ]
    }
   ],
   "source": [
    "# ------------------Data Initialization----------------------#\n",
    "\n",
    "# convert .dat file to time-sorted pandas dataframe\n",
    "ml_1m = create_df(read_filename,size=size)\n",
    "\n",
    "# remove users who have sessions lengths less than min_len\n",
    "ml_1m = filter_df(ml_1m,item_min=min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n"
     ]
    }
   ],
   "source": [
    "# ------------------Data Initialization----------------------#\n",
    "if read_movie_filename != \"\":\n",
    "    ml_movie_df = create_movie_df(read_movie_filename,size=size)\n",
    "    ml_movie_df = convert_genres(ml_movie_df)\n",
    "    \n",
    "    # initialize reset object\n",
    "    reset_object = reset_df()\n",
    "    \n",
    "    # map all user ids, item ids, and genres to range 0 - number of users/items/genres\n",
    "    ml_1m,ml_movie_df = reset_object.fit_transform(ml_1m,ml_movie_df)\n",
    "    \n",
    "    # value that padded genre tokens shall take\n",
    "    pad_genre_token = reset_object.genre_enc.transform([\"NULL\"]).item()\n",
    "    \n",
    "    genre_dim = len(np.unique(np.concatenate(ml_movie_df.genre))) - 1\n",
    "\n",
    "else:\n",
    "    # initialize reset object\n",
    "    reset_object = reset_df()\n",
    "    \n",
    "    # map all user ids and item ids to range 0 - Number of Users/Items \n",
    "    # i.e. [1,7,5] -> [0,2,1]\n",
    "    ml_1m = reset_object.fit_transform(ml_1m)\n",
    "    \n",
    "    pad_genre_token = None\n",
    "    ml_movie_df = None\n",
    "    genre_dim = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------Data Initialization----------------------#\n",
    "# how many unique users, items, ratings and timestamps are there\n",
    "n_users,n_items,n_ratings,n_timestamp = ml_1m.nunique()\n",
    "\n",
    "# value that padded tokens shall take\n",
    "pad_token = n_items\n",
    "\n",
    "# the output dimension for softmax layer\n",
    "output_dim = n_items\n",
    "\n",
    "\n",
    "# get the item id : bert plot embedding dictionary\n",
    "if bert_dim != 0:\n",
    "    feature_embed = bert2dict(bert_filename=read_bert_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                              | 82/6040 [00:00<00:07, 818.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Creating User Histories ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6040/6040 [00:06<00:00, 920.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of every user's session (history)\n",
    "# i.e. {user: [user clicks]}\n",
    "if size == \"1m\":\n",
    "    user_history = create_user_history(ml_1m)\n",
    "\n",
    "elif size == \"20m\":\n",
    "    import pickle\n",
    "    with open('userhistory.pickle', 'rb') as handle:\n",
    "        user_history = pickle.load(handle)\n",
    "# create a dictionary of all items a user has not clicked\n",
    "# i.e. {user: [items not clicked by user]}\n",
    "# user_noclicks = create_user_noclick(user_history,ml_1m,n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "#with open('userhistory.pickle', 'wb') as handle:\n",
    "#    pickle.dump(user_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('userhistory.pickle', 'rb') as handle:\n",
    "#    user_history = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 6040/6040 [00:00<00:00, 208828.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Splitting User Histories into Train, Validation, and Test Splits ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# split data by leave-one-out strategy\n",
    "# have train dictionary {user: [last 41 items prior to last 2 items in user session]}\n",
    "# have val dictionary {user: [last 41 items prior to last item in user session]}\n",
    "# have test dictionary {user: [last 41 items]}\n",
    "# i.e. if max_length = 4, [1,2,3,4,5,6] -> [1,2,3,4] , [2,3,4,5] , [3,4,5,6]\n",
    "train_history,val_history,test_history = train_val_test_split(user_history,max_length=max_length)\n",
    "\n",
    "# initialize the train,validation, and test pytorch dataset objects\n",
    "# eval pads all items except last token to predict\n",
    "train_dataset = GRUDataset(train_history,genre_df=ml_movie_df,mode='train',max_length=max_length,pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
    "val_dataset = GRUDataset(val_history,genre_df=ml_movie_df,mode='eval',max_length=max_length,pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
    "test_dataset = GRUDataset(test_history,genre_df=ml_movie_df,mode='eval',max_length=max_length,pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
    "\n",
    "# create the train,validation, and test pytorch dataloader objects\n",
    "train_dl = DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
    "val_dl = DataLoader(val_dataset,batch_size=64)\n",
    "test_dl = DataLoader(test_dataset,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert dim: 0\n",
      "Genre dim: 0\n",
      "Pad Token: 3706\n",
      "Pad Genre Token: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Bert dim: {:d}\".format(bert_dim))\n",
    "print(\"Genre dim: {:d}\".format(genre_dim))\n",
    "print(\"Pad Token: {}\".format(pad_token))\n",
    "print(\"Pad Genre Token: {}\".format(pad_genre_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------Model Initialization----------------------#\n",
    "\n",
    "# initialize gru4rec model with arguments specified earlier\n",
    "if model_type == \"feature_add\":\n",
    "    model = gru4recF(embedding_dim=embedding_dim,\n",
    "             hidden_dim=hidden_dim,\n",
    "             output_dim=output_dim,\n",
    "             genre_dim=genre_dim,\n",
    "             batch_first=True,\n",
    "             max_length=max_length,\n",
    "             pad_token=pad_token,\n",
    "             pad_genre_token=pad_genre_token,\n",
    "             bert_dim=bert_dim,\n",
    "             tied = tied,\n",
    "             dropout=dropout)\n",
    "\n",
    "\n",
    "if model_type == \"feature_concat\":\n",
    "    model = gru4recFC(embedding_dim=embedding_dim,\n",
    "             hidden_dim=hidden_dim,\n",
    "             output_dim=output_dim,\n",
    "             genre_dim=genre_dim,\n",
    "             batch_first=True,\n",
    "             max_length=max_length,\n",
    "             pad_token=pad_token,\n",
    "             pad_genre_token=pad_genre_token,\n",
    "             bert_dim=bert_dim,\n",
    "             tied = tied,\n",
    "             dropout=dropout)\n",
    "\n",
    "if model_type == \"vanilla\":\n",
    "    model = gru4rec_vanilla(hidden_dim=hidden_dim,\n",
    "                            output_dim=output_dim,\n",
    "                            batch_first=True,\n",
    "                            max_length=max_length,\n",
    "                            pad_token=pad_token,\n",
    "                            tied=tied,\n",
    "                            embedding_dim=embedding_dim)\n",
    "\n",
    "if model_type ==\"feature_only\":\n",
    "    model = gru4rec_feature(hidden_dim=hidden_dim,\n",
    "                            output_dim=output_dim,\n",
    "                            batch_first=True,\n",
    "                            max_length=max_length,\n",
    "                            pad_token=pad_token,\n",
    "                            bert_dim=bert_dim)\n",
    "\n",
    "if model_type == \"conv\":\n",
    "    model = gru4rec_conv(embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 batch_first=True,\n",
    "                 max_length=200,\n",
    "                 pad_token=0,\n",
    "                 dropout=0,\n",
    "                 window=3,\n",
    "                 tied=tied)\n",
    "    \n",
    "if model_type == \"nextitnet\":\n",
    "    model = NextItNet(embedding_dim=embedding_dim,\n",
    "                      output_dim=output_dim,\n",
    "                      hidden_layers=hidden_layers,\n",
    "                      dilations=dilations,\n",
    "                      pad_token=n_items,\n",
    "                      max_len=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bert_dim != 0:\n",
    "    model.init_weight(reset_object,feature_embed)\n",
    "    \n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_embedding.weight',\n",
       " 'hidden_layers.0.0.reduce.weight',\n",
       " 'hidden_layers.0.0.reduce.bias',\n",
       " 'hidden_layers.0.0.masked.weight',\n",
       " 'hidden_layers.0.0.masked.bias',\n",
       " 'hidden_layers.0.0.increase.weight',\n",
       " 'hidden_layers.0.0.increase.bias',\n",
       " 'hidden_layers.0.0.reduce_norm.weight',\n",
       " 'hidden_layers.0.0.reduce_norm.bias',\n",
       " 'hidden_layers.0.0.masked_norm.weight',\n",
       " 'hidden_layers.0.0.masked_norm.bias',\n",
       " 'hidden_layers.0.0.increase_norm.weight',\n",
       " 'hidden_layers.0.0.increase_norm.bias',\n",
       " 'hidden_layers.0.1.reduce.weight',\n",
       " 'hidden_layers.0.1.reduce.bias',\n",
       " 'hidden_layers.0.1.masked.weight',\n",
       " 'hidden_layers.0.1.masked.bias',\n",
       " 'hidden_layers.0.1.increase.weight',\n",
       " 'hidden_layers.0.1.increase.bias',\n",
       " 'hidden_layers.0.1.reduce_norm.weight',\n",
       " 'hidden_layers.0.1.reduce_norm.bias',\n",
       " 'hidden_layers.0.1.masked_norm.weight',\n",
       " 'hidden_layers.0.1.masked_norm.bias',\n",
       " 'hidden_layers.0.1.increase_norm.weight',\n",
       " 'hidden_layers.0.1.increase_norm.bias',\n",
       " 'hidden_layers.0.2.reduce.weight',\n",
       " 'hidden_layers.0.2.reduce.bias',\n",
       " 'hidden_layers.0.2.masked.weight',\n",
       " 'hidden_layers.0.2.masked.bias',\n",
       " 'hidden_layers.0.2.increase.weight',\n",
       " 'hidden_layers.0.2.increase.bias',\n",
       " 'hidden_layers.0.2.reduce_norm.weight',\n",
       " 'hidden_layers.0.2.reduce_norm.bias',\n",
       " 'hidden_layers.0.2.masked_norm.weight',\n",
       " 'hidden_layers.0.2.masked_norm.bias',\n",
       " 'hidden_layers.0.2.increase_norm.weight',\n",
       " 'hidden_layers.0.2.increase_norm.bias',\n",
       " 'hidden_layers.0.3.reduce.weight',\n",
       " 'hidden_layers.0.3.reduce.bias',\n",
       " 'hidden_layers.0.3.masked.weight',\n",
       " 'hidden_layers.0.3.masked.bias',\n",
       " 'hidden_layers.0.3.increase.weight',\n",
       " 'hidden_layers.0.3.increase.bias',\n",
       " 'hidden_layers.0.3.reduce_norm.weight',\n",
       " 'hidden_layers.0.3.reduce_norm.bias',\n",
       " 'hidden_layers.0.3.masked_norm.weight',\n",
       " 'hidden_layers.0.3.masked_norm.bias',\n",
       " 'hidden_layers.0.3.increase_norm.weight',\n",
       " 'hidden_layers.0.3.increase_norm.bias',\n",
       " 'hidden_layers.1.0.reduce.weight',\n",
       " 'hidden_layers.1.0.reduce.bias',\n",
       " 'hidden_layers.1.0.masked.weight',\n",
       " 'hidden_layers.1.0.masked.bias',\n",
       " 'hidden_layers.1.0.increase.weight',\n",
       " 'hidden_layers.1.0.increase.bias',\n",
       " 'hidden_layers.1.0.reduce_norm.weight',\n",
       " 'hidden_layers.1.0.reduce_norm.bias',\n",
       " 'hidden_layers.1.0.masked_norm.weight',\n",
       " 'hidden_layers.1.0.masked_norm.bias',\n",
       " 'hidden_layers.1.0.increase_norm.weight',\n",
       " 'hidden_layers.1.0.increase_norm.bias',\n",
       " 'hidden_layers.1.1.reduce.weight',\n",
       " 'hidden_layers.1.1.reduce.bias',\n",
       " 'hidden_layers.1.1.masked.weight',\n",
       " 'hidden_layers.1.1.masked.bias',\n",
       " 'hidden_layers.1.1.increase.weight',\n",
       " 'hidden_layers.1.1.increase.bias',\n",
       " 'hidden_layers.1.1.reduce_norm.weight',\n",
       " 'hidden_layers.1.1.reduce_norm.bias',\n",
       " 'hidden_layers.1.1.masked_norm.weight',\n",
       " 'hidden_layers.1.1.masked_norm.bias',\n",
       " 'hidden_layers.1.1.increase_norm.weight',\n",
       " 'hidden_layers.1.1.increase_norm.bias',\n",
       " 'hidden_layers.1.2.reduce.weight',\n",
       " 'hidden_layers.1.2.reduce.bias',\n",
       " 'hidden_layers.1.2.masked.weight',\n",
       " 'hidden_layers.1.2.masked.bias',\n",
       " 'hidden_layers.1.2.increase.weight',\n",
       " 'hidden_layers.1.2.increase.bias',\n",
       " 'hidden_layers.1.2.reduce_norm.weight',\n",
       " 'hidden_layers.1.2.reduce_norm.bias',\n",
       " 'hidden_layers.1.2.masked_norm.weight',\n",
       " 'hidden_layers.1.2.masked_norm.bias',\n",
       " 'hidden_layers.1.2.increase_norm.weight',\n",
       " 'hidden_layers.1.2.increase_norm.bias',\n",
       " 'hidden_layers.1.3.reduce.weight',\n",
       " 'hidden_layers.1.3.reduce.bias',\n",
       " 'hidden_layers.1.3.masked.weight',\n",
       " 'hidden_layers.1.3.masked.bias',\n",
       " 'hidden_layers.1.3.increase.weight',\n",
       " 'hidden_layers.1.3.increase.bias',\n",
       " 'hidden_layers.1.3.reduce_norm.weight',\n",
       " 'hidden_layers.1.3.reduce_norm.bias',\n",
       " 'hidden_layers.1.3.masked_norm.weight',\n",
       " 'hidden_layers.1.3.masked_norm.bias',\n",
       " 'hidden_layers.1.3.increase_norm.weight',\n",
       " 'hidden_layers.1.3.increase_norm.bias',\n",
       " 'hidden_layers.2.0.reduce.weight',\n",
       " 'hidden_layers.2.0.reduce.bias',\n",
       " 'hidden_layers.2.0.masked.weight',\n",
       " 'hidden_layers.2.0.masked.bias',\n",
       " 'hidden_layers.2.0.increase.weight',\n",
       " 'hidden_layers.2.0.increase.bias',\n",
       " 'hidden_layers.2.0.reduce_norm.weight',\n",
       " 'hidden_layers.2.0.reduce_norm.bias',\n",
       " 'hidden_layers.2.0.masked_norm.weight',\n",
       " 'hidden_layers.2.0.masked_norm.bias',\n",
       " 'hidden_layers.2.0.increase_norm.weight',\n",
       " 'hidden_layers.2.0.increase_norm.bias',\n",
       " 'hidden_layers.2.1.reduce.weight',\n",
       " 'hidden_layers.2.1.reduce.bias',\n",
       " 'hidden_layers.2.1.masked.weight',\n",
       " 'hidden_layers.2.1.masked.bias',\n",
       " 'hidden_layers.2.1.increase.weight',\n",
       " 'hidden_layers.2.1.increase.bias',\n",
       " 'hidden_layers.2.1.reduce_norm.weight',\n",
       " 'hidden_layers.2.1.reduce_norm.bias',\n",
       " 'hidden_layers.2.1.masked_norm.weight',\n",
       " 'hidden_layers.2.1.masked_norm.bias',\n",
       " 'hidden_layers.2.1.increase_norm.weight',\n",
       " 'hidden_layers.2.1.increase_norm.bias',\n",
       " 'hidden_layers.2.2.reduce.weight',\n",
       " 'hidden_layers.2.2.reduce.bias',\n",
       " 'hidden_layers.2.2.masked.weight',\n",
       " 'hidden_layers.2.2.masked.bias',\n",
       " 'hidden_layers.2.2.increase.weight',\n",
       " 'hidden_layers.2.2.increase.bias',\n",
       " 'hidden_layers.2.2.reduce_norm.weight',\n",
       " 'hidden_layers.2.2.reduce_norm.bias',\n",
       " 'hidden_layers.2.2.masked_norm.weight',\n",
       " 'hidden_layers.2.2.masked_norm.bias',\n",
       " 'hidden_layers.2.2.increase_norm.weight',\n",
       " 'hidden_layers.2.2.increase_norm.bias',\n",
       " 'hidden_layers.2.3.reduce.weight',\n",
       " 'hidden_layers.2.3.reduce.bias',\n",
       " 'hidden_layers.2.3.masked.weight',\n",
       " 'hidden_layers.2.3.masked.bias',\n",
       " 'hidden_layers.2.3.increase.weight',\n",
       " 'hidden_layers.2.3.increase.bias',\n",
       " 'hidden_layers.2.3.reduce_norm.weight',\n",
       " 'hidden_layers.2.3.reduce_norm.bias',\n",
       " 'hidden_layers.2.3.masked_norm.weight',\n",
       " 'hidden_layers.2.3.masked_norm.bias',\n",
       " 'hidden_layers.2.3.increase_norm.weight',\n",
       " 'hidden_layers.2.3.increase_norm.bias',\n",
       " 'final_layer.weight',\n",
       " 'final_layer.bias']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name,param in model.named_parameters() if ((\"movie\" not in name) or (\"plot_embedding\" in name) or (\"genre\" in name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_embedding.weight',\n",
       " 'hidden_layers.0.0.reduce.weight',\n",
       " 'hidden_layers.0.0.reduce.bias',\n",
       " 'hidden_layers.0.0.masked.weight',\n",
       " 'hidden_layers.0.0.masked.bias',\n",
       " 'hidden_layers.0.0.increase.weight',\n",
       " 'hidden_layers.0.0.increase.bias',\n",
       " 'hidden_layers.0.0.reduce_norm.weight',\n",
       " 'hidden_layers.0.0.reduce_norm.bias',\n",
       " 'hidden_layers.0.0.masked_norm.weight',\n",
       " 'hidden_layers.0.0.masked_norm.bias',\n",
       " 'hidden_layers.0.0.increase_norm.weight',\n",
       " 'hidden_layers.0.0.increase_norm.bias',\n",
       " 'hidden_layers.0.1.reduce.weight',\n",
       " 'hidden_layers.0.1.reduce.bias',\n",
       " 'hidden_layers.0.1.masked.weight',\n",
       " 'hidden_layers.0.1.masked.bias',\n",
       " 'hidden_layers.0.1.increase.weight',\n",
       " 'hidden_layers.0.1.increase.bias',\n",
       " 'hidden_layers.0.1.reduce_norm.weight',\n",
       " 'hidden_layers.0.1.reduce_norm.bias',\n",
       " 'hidden_layers.0.1.masked_norm.weight',\n",
       " 'hidden_layers.0.1.masked_norm.bias',\n",
       " 'hidden_layers.0.1.increase_norm.weight',\n",
       " 'hidden_layers.0.1.increase_norm.bias',\n",
       " 'hidden_layers.0.2.reduce.weight',\n",
       " 'hidden_layers.0.2.reduce.bias',\n",
       " 'hidden_layers.0.2.masked.weight',\n",
       " 'hidden_layers.0.2.masked.bias',\n",
       " 'hidden_layers.0.2.increase.weight',\n",
       " 'hidden_layers.0.2.increase.bias',\n",
       " 'hidden_layers.0.2.reduce_norm.weight',\n",
       " 'hidden_layers.0.2.reduce_norm.bias',\n",
       " 'hidden_layers.0.2.masked_norm.weight',\n",
       " 'hidden_layers.0.2.masked_norm.bias',\n",
       " 'hidden_layers.0.2.increase_norm.weight',\n",
       " 'hidden_layers.0.2.increase_norm.bias',\n",
       " 'hidden_layers.0.3.reduce.weight',\n",
       " 'hidden_layers.0.3.reduce.bias',\n",
       " 'hidden_layers.0.3.masked.weight',\n",
       " 'hidden_layers.0.3.masked.bias',\n",
       " 'hidden_layers.0.3.increase.weight',\n",
       " 'hidden_layers.0.3.increase.bias',\n",
       " 'hidden_layers.0.3.reduce_norm.weight',\n",
       " 'hidden_layers.0.3.reduce_norm.bias',\n",
       " 'hidden_layers.0.3.masked_norm.weight',\n",
       " 'hidden_layers.0.3.masked_norm.bias',\n",
       " 'hidden_layers.0.3.increase_norm.weight',\n",
       " 'hidden_layers.0.3.increase_norm.bias',\n",
       " 'hidden_layers.1.0.reduce.weight',\n",
       " 'hidden_layers.1.0.reduce.bias',\n",
       " 'hidden_layers.1.0.masked.weight',\n",
       " 'hidden_layers.1.0.masked.bias',\n",
       " 'hidden_layers.1.0.increase.weight',\n",
       " 'hidden_layers.1.0.increase.bias',\n",
       " 'hidden_layers.1.0.reduce_norm.weight',\n",
       " 'hidden_layers.1.0.reduce_norm.bias',\n",
       " 'hidden_layers.1.0.masked_norm.weight',\n",
       " 'hidden_layers.1.0.masked_norm.bias',\n",
       " 'hidden_layers.1.0.increase_norm.weight',\n",
       " 'hidden_layers.1.0.increase_norm.bias',\n",
       " 'hidden_layers.1.1.reduce.weight',\n",
       " 'hidden_layers.1.1.reduce.bias',\n",
       " 'hidden_layers.1.1.masked.weight',\n",
       " 'hidden_layers.1.1.masked.bias',\n",
       " 'hidden_layers.1.1.increase.weight',\n",
       " 'hidden_layers.1.1.increase.bias',\n",
       " 'hidden_layers.1.1.reduce_norm.weight',\n",
       " 'hidden_layers.1.1.reduce_norm.bias',\n",
       " 'hidden_layers.1.1.masked_norm.weight',\n",
       " 'hidden_layers.1.1.masked_norm.bias',\n",
       " 'hidden_layers.1.1.increase_norm.weight',\n",
       " 'hidden_layers.1.1.increase_norm.bias',\n",
       " 'hidden_layers.1.2.reduce.weight',\n",
       " 'hidden_layers.1.2.reduce.bias',\n",
       " 'hidden_layers.1.2.masked.weight',\n",
       " 'hidden_layers.1.2.masked.bias',\n",
       " 'hidden_layers.1.2.increase.weight',\n",
       " 'hidden_layers.1.2.increase.bias',\n",
       " 'hidden_layers.1.2.reduce_norm.weight',\n",
       " 'hidden_layers.1.2.reduce_norm.bias',\n",
       " 'hidden_layers.1.2.masked_norm.weight',\n",
       " 'hidden_layers.1.2.masked_norm.bias',\n",
       " 'hidden_layers.1.2.increase_norm.weight',\n",
       " 'hidden_layers.1.2.increase_norm.bias',\n",
       " 'hidden_layers.1.3.reduce.weight',\n",
       " 'hidden_layers.1.3.reduce.bias',\n",
       " 'hidden_layers.1.3.masked.weight',\n",
       " 'hidden_layers.1.3.masked.bias',\n",
       " 'hidden_layers.1.3.increase.weight',\n",
       " 'hidden_layers.1.3.increase.bias',\n",
       " 'hidden_layers.1.3.reduce_norm.weight',\n",
       " 'hidden_layers.1.3.reduce_norm.bias',\n",
       " 'hidden_layers.1.3.masked_norm.weight',\n",
       " 'hidden_layers.1.3.masked_norm.bias',\n",
       " 'hidden_layers.1.3.increase_norm.weight',\n",
       " 'hidden_layers.1.3.increase_norm.bias',\n",
       " 'hidden_layers.2.0.reduce.weight',\n",
       " 'hidden_layers.2.0.reduce.bias',\n",
       " 'hidden_layers.2.0.masked.weight',\n",
       " 'hidden_layers.2.0.masked.bias',\n",
       " 'hidden_layers.2.0.increase.weight',\n",
       " 'hidden_layers.2.0.increase.bias',\n",
       " 'hidden_layers.2.0.reduce_norm.weight',\n",
       " 'hidden_layers.2.0.reduce_norm.bias',\n",
       " 'hidden_layers.2.0.masked_norm.weight',\n",
       " 'hidden_layers.2.0.masked_norm.bias',\n",
       " 'hidden_layers.2.0.increase_norm.weight',\n",
       " 'hidden_layers.2.0.increase_norm.bias',\n",
       " 'hidden_layers.2.1.reduce.weight',\n",
       " 'hidden_layers.2.1.reduce.bias',\n",
       " 'hidden_layers.2.1.masked.weight',\n",
       " 'hidden_layers.2.1.masked.bias',\n",
       " 'hidden_layers.2.1.increase.weight',\n",
       " 'hidden_layers.2.1.increase.bias',\n",
       " 'hidden_layers.2.1.reduce_norm.weight',\n",
       " 'hidden_layers.2.1.reduce_norm.bias',\n",
       " 'hidden_layers.2.1.masked_norm.weight',\n",
       " 'hidden_layers.2.1.masked_norm.bias',\n",
       " 'hidden_layers.2.1.increase_norm.weight',\n",
       " 'hidden_layers.2.1.increase_norm.bias',\n",
       " 'hidden_layers.2.2.reduce.weight',\n",
       " 'hidden_layers.2.2.reduce.bias',\n",
       " 'hidden_layers.2.2.masked.weight',\n",
       " 'hidden_layers.2.2.masked.bias',\n",
       " 'hidden_layers.2.2.increase.weight',\n",
       " 'hidden_layers.2.2.increase.bias',\n",
       " 'hidden_layers.2.2.reduce_norm.weight',\n",
       " 'hidden_layers.2.2.reduce_norm.bias',\n",
       " 'hidden_layers.2.2.masked_norm.weight',\n",
       " 'hidden_layers.2.2.masked_norm.bias',\n",
       " 'hidden_layers.2.2.increase_norm.weight',\n",
       " 'hidden_layers.2.2.increase_norm.bias',\n",
       " 'hidden_layers.2.3.reduce.weight',\n",
       " 'hidden_layers.2.3.reduce.bias',\n",
       " 'hidden_layers.2.3.masked.weight',\n",
       " 'hidden_layers.2.3.masked.bias',\n",
       " 'hidden_layers.2.3.increase.weight',\n",
       " 'hidden_layers.2.3.increase.bias',\n",
       " 'hidden_layers.2.3.reduce_norm.weight',\n",
       " 'hidden_layers.2.3.reduce_norm.bias',\n",
       " 'hidden_layers.2.3.masked_norm.weight',\n",
       " 'hidden_layers.2.3.masked_norm.bias',\n",
       " 'hidden_layers.2.3.increase_norm.weight',\n",
       " 'hidden_layers.2.3.increase_norm.bias',\n",
       " 'final_layer.weight',\n",
       " 'final_layer.bias']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name,param in model.named_parameters() if (\"plot\" not in name) and (\"genre\" not in name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Adam optimizer with gru4rec model parameters\n",
    "if train_method != \"normal\":\n",
    "    optimizer_features = torch.optim.Adam([param for name,param in model.named_parameters() if ((\"movie\" not in name) or (\"plot_embedding\" in name) or (\"genre\" in name)) ],\n",
    "                                          lr=lr/10,weight_decay=reg)\n",
    "    \n",
    "    optimizer_ids = torch.optim.Adam([param for name,param in model.named_parameters() if (\"plot\" not in name) and (\"genre\" not in name)],\n",
    "                                     lr=lr,weight_decay=reg)\n",
    "\n",
    "elif train_method == \"normal\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=reg)\n",
    "    \n",
    "if freeze_plot and bert_dim !=0:\n",
    "    model.plot_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=n_items)\n",
    "#Recall_Object = Recall_E_prob(ml_1m,user_history,n_users,n_items,k=k)\n",
    "#Recall_Object = Recall_E_Noprob(ml_1m,user_history,n_users,n_items,k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Creating Hit@10 Metric Object ==========\n"
     ]
    }
   ],
   "source": [
    "Recall_Object = Recall_E_prob(ml_1m,user_history,n_users,n_items,k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Baseline POP results: \",Recall_Object.popular_baseline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_hit = Recall_Object(model,train_dl)\n",
    "#validation_hit = Recall_Object(model,val_dl)\n",
    "#testing_hit = Recall_Object(model,test_dl)\n",
    "#print(\"Training Hits@{:d}: {:.2f}\".format(k,training_hit))\n",
    "#print(\"Validation Hits@{:d}: {:.2f}\".format(k,validation_hit))\n",
    "#print(\"Testing Hits@{:d}: {:.2f}\".format(k,testing_hit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nextitnet'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch 1 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-22-272a4c62a179>\", line 28, in <module>\n",
      "    outputs = model(x=inputs.cuda(),x_lens=x_lens.squeeze().tolist())\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'x_lens'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-22-272a4c62a179>\", line 28, in <module>\n",
      "    outputs = model(x=inputs.cuda(),x_lens=x_lens.squeeze().tolist())\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'x_lens'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3434, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-22-272a4c62a179>\", line 28, in <module>\n",
      "    outputs = model(x=inputs.cuda(),x_lens=x_lens.squeeze().tolist())\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'x_lens'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3434, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2922, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3356, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1211, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\lpott\\anaconda3\\envs\\NLP\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "# ------------------Training Initialization----------------------#\n",
    "max_train_hit = 0\n",
    "max_val_hit = 0\n",
    "max_test_hit = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"=\"*20,\"Epoch {}\".format(epoch+1),\"=\"*20)\n",
    "    \n",
    "    model.train()  \n",
    "    \n",
    "    running_loss = 0\n",
    "\n",
    "    for j,data in enumerate(tqdm(train_dl,position=0,leave=True)):\n",
    "        \n",
    "        if train_method != \"normal\":\n",
    "            optimizer_features.zero_grad()\n",
    "            optimizer_ids.zero_grad()\n",
    "            \n",
    "        elif train_method == \"normal\": \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        if genre_dim != 0:            \n",
    "            inputs,genre_inputs,labels,x_lens,uid = data\n",
    "            outputs = model(x=inputs.cuda(),x_lens=x_lens.squeeze().tolist(),x_genre=genre_inputs.cuda())\n",
    "        \n",
    "        elif genre_dim == 0:\n",
    "            inputs,labels,x_lens,uid = data \n",
    "            outputs = model(x=inputs.cuda(),x_lens=x_lens.squeeze().tolist())\n",
    "       \n",
    "        if tied:\n",
    "            outputs_ignore_pad = outputs[:,:,:-1]\n",
    "            loss = loss_fn(outputs_ignore_pad.view(-1,outputs_ignore_pad.size(-1)),labels.view(-1).cuda())\n",
    "            \n",
    "        else:\n",
    "            loss = loss_fn(outputs.view(-1,outputs.size(-1)),labels.view(-1).cuda())\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        if train_method != \"normal\":\n",
    "            if train_method == \"interleave\":\n",
    "                # interleave on the epochs\n",
    "                if (j+1) % 2 == 0:\n",
    "                    optimizer_features.step()\n",
    "                else:\n",
    "                    optimizer_ids.step()\n",
    "\n",
    "            elif train_method == \"alternate\":\n",
    "                if (epoch+1) % 2 == 0:\n",
    "                    optimizer_features.step()\n",
    "                else:\n",
    "                    optimizer_ids.step()\n",
    "\n",
    "        elif train_method == \"normal\":\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.detach().cpu().item()\n",
    "\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    training_hit = Recall_Object(model,train_dl,\"train\")\n",
    "    validation_hit = Recall_Object(model,val_dl,\"validation\")\n",
    "    testing_hit = Recall_Object(model,test_dl,\"test\")\n",
    "    \n",
    "    if max_val_hit < validation_hit:\n",
    "        max_val_hit = validation_hit\n",
    "        max_test_hit = testing_hit\n",
    "        max_train_hit = training_hit\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training CE Loss: {:.5f}\".format(running_loss/len(train_dl)))\n",
    "    print(\"Training Hits@{:d}: {:.2f}\".format(k,training_hit))\n",
    "    print(\"Validation Hits@{:d}: {:.2f}\".format(k,validation_hit))\n",
    "    print(\"Testing Hits@{:d}: {:.2f}\".format(k,testing_hit))\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Maximum Training Hit@{:d}: {:.2f}\".format(k,max_train_hit))\n",
    "print(\"Maximum Validation Hit@{:d}: {:.2f}\".format(k,max_val_hit))\n",
    "print(\"Maximum Testing Hit@{:d}: {:.2f}\".format(k,max_test_hit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Maximum Training Hit@10: 0.00\n",
      "Maximum Validation Hit@10: 0.00\n",
      "Maximum Testing Hit@10: 0.00\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 4224\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Maximum Training Hit@{:d}: {:.2f}\".format(k,max_train_hit))\n",
    "print(\"Maximum Validation Hit@{:d}: {:.2f}\".format(k,max_val_hit))\n",
    "print(\"Maximum Testing Hit@{:d}: {:.2f}\".format(k,max_test_hit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
