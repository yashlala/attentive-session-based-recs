{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "GRU4REC-GridSearch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed4PpylblUOR"
      },
      "source": [
        "# GRU4REC Grid Search II\n",
        "\n",
        "This notebook tests various GRU4REC models over a set of hyperparameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xet0SAWsljpY",
        "outputId": "2e27f2f4-699c-4477-87c1-245cd801c129"
      },
      "source": [
        "# Environment Setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        " \n",
        "# Import local modules\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMqr669ylzvA",
        "outputId": "939947b1-c1c5-41d6-d5bc-30971faba793"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 2.8MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JQzkF53lUOS"
      },
      "source": [
        "# Tested Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s31WZ9jilUOT"
      },
      "source": [
        "# All Hyperparameters in the Spreadsheet.\n",
        "\n",
        "num_epochs_all = [ 50, 70 ]\n",
        "lr_all = [ 0.01, 0.001, 0.005, 0.0001]\n",
        "# used when using two optimizers (\"alternate\" training method). \n",
        "# Usually set to lr / 10, but try tweaking it. \n",
        "lr_alt_all = [ 0.001, 0.0001, 0.0005, 0.00001]\n",
        "batch_size =  64\n",
        "reg_all = [ 5e-4 ]\n",
        "bpr_reg_all = [ 0.8 ]\n",
        "num_neg_samples_all = [ 25 ]\n",
        "train_method_all = [ \"alternate\" ]\n",
        "hidden_dim_all = [ 256 ]\n",
        "embedding_dim_all = [ 256 ] \n",
        "bert_dim_all = [ 768 ]\n",
        "# : ???\n",
        "max_length = 200 \n",
        "freeze_plot_all = [ False ] \n",
        "tied_all = [ False ] \n",
        "loss_type_all = [ \"BPR\" ] \n",
        "dilations_all = [ (1,2,2,4) ] # Only used for cross entropy.\n",
        "\n",
        "# Hyperparameters not in the Spreadsheet: \n",
        "\n",
        "window = 3\n",
        "dropout= 0\n",
        "k = 10\n",
        "min_len = 10\n",
        "\n",
        "# NextItNet options. \n",
        "hidden_layers = 3\n",
        "model_type = \"feature_add\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwwKRSuplUOU"
      },
      "source": [
        "# Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuNz9ijTlUOV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from preprocessing import *\n",
        "from dataset import *\n",
        "from metrics import *\n",
        "from model import *\n",
        "from utils import bert2dict"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyjKPDvMlUOW"
      },
      "source": [
        "read_filename =\"/content/gdrive/My Drive/Colab Notebooks/movielens-1m/ratings.dat\"\n",
        "read_bert_filename = \"/content/gdrive/My Drive/Colab Notebooks/bert_sequence_20m.txt\"\n",
        "read_movie_filename = \"\" \n",
        "size = \"1m\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VIMbNwSlUOX",
        "outputId": "027fcd3a-fffe-4dde-89fb-71af16ccd8bc"
      },
      "source": [
        "# ------------------Data Initialization----------------------#\n",
        "# convert .dat file to time-sorted pandas dataframe\n",
        "ml_1m = create_df(read_filename, size=size)\n",
        "\n",
        "# remove users who have session lengths less than min_len\n",
        "ml_1m = filter_df(ml_1m, item_min=min_len)\n",
        "\n",
        "if read_movie_filename != \"\":\n",
        "    ml_movie_df = create_movie_df(read_movie_filename,size=size)\n",
        "    ml_movie_df = convert_genres(ml_movie_df)\n",
        "    \n",
        "    # initialize reset object\n",
        "    reset_object = reset_df()\n",
        "    \n",
        "    # map all user ids, item ids, and genres to range 0 - number of users/items/genres\n",
        "    ml_1m,ml_movie_df = reset_object.fit_transform(ml_1m,ml_movie_df)\n",
        "    \n",
        "    # value that padded genre tokens shall take\n",
        "    pad_genre_token = reset_object.genre_enc.transform([\"NULL\"]).item()\n",
        "    \n",
        "    genre_dim = len(np.unique(np.concatenate(ml_movie_df.genre))) - 1\n",
        "\n",
        "else:\n",
        "    # initialize reset object\n",
        "    reset_object = reset_df()\n",
        "    \n",
        "    # map all user ids and item ids to range 0 - Number of Users/Items \n",
        "    # i.e. [1,7,5] -> [0,2,1]\n",
        "    ml_1m = reset_object.fit_transform(ml_1m)\n",
        "    \n",
        "    pad_genre_token = None\n",
        "    ml_movie_df = None\n",
        "    genre_dim = 0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== Creating DataFrame ==========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/preprocessing.py:32: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  df = pd.read_csv(filename,sep='::',header=None)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "user_id        6040\n",
            "item_id        3706\n",
            "rating            5\n",
            "timestamp    458455\n",
            "dtype: int64\n",
            "(1000209, 4)\n",
            "Minimum Session Length: 20\n",
            "Maximum Session Length: 2314\n",
            "Average Session Length: 165.60\n",
            "========== Filtering Sessions <= 10  DataFrame ==========\n",
            "user_id        6040\n",
            "item_id        3706\n",
            "rating            5\n",
            "timestamp    458455\n",
            "dtype: int64\n",
            "(1000209, 4)\n",
            "Minimum Session Length: 20\n",
            "Maximum Session Length: 2314\n",
            "Average Session Length: 165.60\n",
            "========== Initialize Reset DataFrame Object ==========\n",
            "========== Resetting user ids and item ids in DataFrame ==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUz6sDxZlUOZ",
        "outputId": "1b6eff89-308c-4c74-8715-0b5905138686"
      },
      "source": [
        "# ------------------Data Initialization----------------------#\n",
        "# how many unique users, items, ratings and timestamps are there\n",
        "n_users, n_items, n_ratings, n_timestamp = ml_1m.nunique()\n",
        "\n",
        "# value that padded tokens shall take\n",
        "pad_token = n_items\n",
        "\n",
        "# the output dimension for softmax layer\n",
        "output_dim = n_items\n",
        "\n",
        "# get the item id : bert plot embedding dictionary\n",
        "feature_embed = bert2dict(bert_filename=read_bert_filename)\n",
        "    \n",
        "# create a dictionary of every user's session (history)\n",
        "# i.e. {user: [user clicks]}\n",
        "user_history = create_user_history(ml_1m)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== Reading .txt file with all item id and embeddings ==========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 43/6040 [00:00<00:14, 425.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========== Creating User Histories ==========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6040/6040 [00:15<00:00, 397.37it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ddGmiQzlUOa",
        "outputId": "cda487a7-2e3e-4de5-cb55-f4e18381acf9"
      },
      "source": [
        "# split data by leave-one-out strategy\n",
        "# have train dictionary {user: [last 41 items prior to last 2 items in user session]}\n",
        "# have val dictionary {user: [last 41 items prior to last item in user session]}\n",
        "# have test dictionary {user: [last 41 items]}\n",
        "# i.e. if max_length = 4, [1,2,3,4,5,6] -> [1,2,3,4] , [2,3,4,5] , [3,4,5,6]\n",
        "train_history,val_history,test_history = train_val_test_split(user_history,max_length=max_length)\n",
        "\n",
        "# initialize the train,validation, and test pytorch dataset objects\n",
        "# eval pads all items except last token to predict\n",
        "train_dataset = GRUDataset(train_history,genre_df=ml_movie_df,mode='train',max_length=max_length,\n",
        "                           pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
        "val_dataset = GRUDataset(val_history,genre_df=ml_movie_df,mode='eval',max_length=max_length,\n",
        "                         pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
        "test_dataset = GRUDataset(test_history,genre_df=ml_movie_df,mode='eval',max_length=max_length,\n",
        "                          pad_token=pad_token,pad_genre_token=pad_genre_token)\n",
        "\n",
        "# create the train,validation, and test pytorch dataloader objects\n",
        "train_dl = DataLoader(train_dataset,batch_size = batch_size,shuffle=True)\n",
        "val_dl = DataLoader(val_dataset,batch_size=64)\n",
        "test_dl = DataLoader(test_dataset,batch_size=64)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6040/6040 [00:00<00:00, 64581.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========== Splitting User Histories into Train, Validation, and Test Splits ==========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVJt3qmzlUOc"
      },
      "source": [
        "# Model Initialization and Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sei73dlklUOd"
      },
      "source": [
        "def initialize_model(model_type, device,\n",
        "             embedding_dim,\n",
        "             hidden_dim,\n",
        "             output_dim,\n",
        "             genre_dim,\n",
        "             bert_dim,\n",
        "             max_length,\n",
        "             tied,\n",
        "             batch_first=True,\n",
        "             pad_token=pad_token,\n",
        "             pad_genre_token=pad_genre_token,\n",
        "             dropout=dropout): \n",
        "    # initialize gru4rec model with arguments specified earlier\n",
        "    if model_type == \"feature_add\":\n",
        "        model = gru4recF(embedding_dim=embedding_dim,\n",
        "             hidden_dim=hidden_dim,\n",
        "             output_dim=output_dim,\n",
        "             genre_dim=genre_dim,\n",
        "             batch_first=True,\n",
        "             max_length=max_length,\n",
        "             pad_token=pad_token,\n",
        "             pad_genre_token=pad_genre_token,\n",
        "             bert_dim=bert_dim,\n",
        "             tied = tied,\n",
        "             dropout=dropout)\n",
        "\n",
        "\n",
        "    if model_type == \"feature_concat\":\n",
        "        model = gru4recFC(embedding_dim=embedding_dim,\n",
        "             hidden_dim=hidden_dim,\n",
        "             output_dim=output_dim,\n",
        "             genre_dim=genre_dim,\n",
        "             batch_first=True,\n",
        "             max_length=max_length,\n",
        "             pad_token=pad_token,\n",
        "             pad_genre_token=pad_genre_token,\n",
        "             bert_dim=bert_dim,\n",
        "             tied = tied,\n",
        "             dropout=dropout)\n",
        "\n",
        "    if model_type == \"vanilla\":\n",
        "        model = gru4rec_vanilla(hidden_dim=hidden_dim,\n",
        "                            output_dim=output_dim,\n",
        "                            batch_first=True,\n",
        "                            max_length=max_length,\n",
        "                            pad_token=pad_token,\n",
        "                            tied=tied,\n",
        "                            embedding_dim=embedding_dim,\n",
        "                           device=device)\n",
        "\n",
        "    if model_type ==\"feature_only\":\n",
        "        model = gru4rec_feature(hidden_dim=hidden_dim,\n",
        "                            output_dim=output_dim,\n",
        "                            batch_first=True,\n",
        "                            max_length=max_length,\n",
        "                            pad_token=pad_token,\n",
        "                            bert_dim=bert_dim)\n",
        "\n",
        "    if model_type == \"conv\":\n",
        "        model = gru4rec_conv(embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 batch_first=True,\n",
        "                 max_length=200,\n",
        "                 pad_token=0,\n",
        "                 dropout=0,\n",
        "                 window=window,\n",
        "                 tied=tied)\n",
        "    \n",
        "    if model_type == \"nextitnet\":\n",
        "        model = NextItNet(embedding_dim=embedding_dim,\n",
        "                      output_dim=output_dim,\n",
        "                      hidden_layers=hidden_layers,\n",
        "                      dilations=dilations,\n",
        "                      pad_token=n_items,\n",
        "                      max_len=max_length)\n",
        "    \n",
        "    if bert_dim != 0:\n",
        "        model.init_weight(reset_object,feature_embed)\n",
        "    \n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqMR0BdLlUOe"
      },
      "source": [
        "# TODO: move tihs somewhere\n",
        "# if freeze_plot and bert_dim != 0:\n",
        "#    model.plot_embedding.weight.requires_grad = False"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQEz10uDlUOe"
      },
      "source": [
        "def initialize_loss_function(loss_type, n_neg_samples, bpr_reg):\n",
        "    if loss_type == \"XE\":\n",
        "        loss_fn = nn.CrossEntropyLoss(ignore_index=n_items)\n",
        "    elif loss_type == \"BPR\":\n",
        "        loss_fn = BPRLoss(user_history = user_history,\n",
        "                      n_items = n_items, \n",
        "                      df = ml_1m,\n",
        "                      device = device, \n",
        "                      samples=num_neg_samples)\n",
        "    elif loss_type == \"BPR_MAX\":\n",
        "        loss_fn = BPRMaxLoss(user_history = user_history,\n",
        "                      n_items = n_items, \n",
        "                      df = ml_1m,\n",
        "                      device = device,\n",
        "                      reg = bpr_reg, \n",
        "                      samples=num_neg_samples)\n",
        "    else: \n",
        "        raise ValueError(\"Unknown Loss Type.\")\n",
        "        \n",
        "    return loss_fn"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve4WYUqXlUOf",
        "outputId": "f4dc0824-229c-416f-9e6a-7c359707a4dd"
      },
      "source": [
        "# Initialize Metric Object\n",
        "Recall_Object = Recall_E_prob(ml_1m,user_history,n_users,n_items,k=k,device=device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== Creating Hit@10 Metric Object ==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1BqdJMEMlUOf"
      },
      "source": [
        "# ------------------Training Initialization----------------------#\n",
        "\n",
        "\n",
        "def record_best_tuple(max_train, max_validation, max_testing, new_train, new_validation, new_testing): \n",
        "    if max_validation[0] < new_validation[0]: \n",
        "        return new_train, new_validation, new_testing\n",
        "    return max_train, max_validation, max_testing\n",
        "\n",
        "\n",
        "def train_model(model, num_epochs, loss_fn, loss_type, train_method, tied, lr, lr_alternate, reg): \n",
        "    max_train_hit = (0,0,0)\n",
        "    max_val_hit = (0,0,0)\n",
        "    max_test_hit = (0,0,0)\n",
        "    max_train_ndcg = (0,0,0)\n",
        "    max_val_ndcg = (0,0,0)\n",
        "    max_test_ndcg = (0,0,0)\n",
        "    max_train_mrr = 0\n",
        "    max_val_mrr = 0\n",
        "    max_test_mrr = 0\n",
        "    \n",
        "    if train_method != \"normal\":\n",
        "        optimizer_features = torch.optim.Adam([param for name, param in model.named_parameters() \n",
        "                                               if ((\"movie\" not in name) or (\"plot_embedding\" in name) \n",
        "                                               or (\"genre\" in name))],\n",
        "                                              lr=lr_alternate,weight_decay=reg)\n",
        "        optimizer_ids = torch.optim.Adam([param for name, param in model.named_parameters() \n",
        "                                          if (\"plot\" not in name) and (\"genre\" not in name)],\n",
        "                                         lr=lr,weight_decay=reg)\n",
        "    else:\n",
        "        optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=reg)\n",
        "    \n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"=\"*20,\"Epoch {}\".format(epoch+1),\"=\"*20)\n",
        "    \n",
        "        model.train()  \n",
        "    \n",
        "        running_loss = 0\n",
        "\n",
        "        for j, data in enumerate(tqdm(train_dl,position=0,leave=True)):\n",
        "            if train_method != \"normal\":\n",
        "                optimizer_features.zero_grad()\n",
        "                optimizer_ids.zero_grad()\n",
        "            else: \n",
        "                optimizer.zero_grad()\n",
        "        \n",
        "            if genre_dim != 0:            \n",
        "                inputs, genre_inputs, labels, x_lens,uid = data\n",
        "                outputs = model(x=inputs.to(device),x_lens=x_lens.squeeze().tolist(),\n",
        "                                x_genre=genre_inputs.to(device))\n",
        "            else:\n",
        "                inputs,labels,x_lens,uid = data \n",
        "                outputs = model(x=inputs.to(device),x_lens=x_lens.squeeze().tolist())\n",
        "       \n",
        "            if tied:\n",
        "                outputs_ignore_pad = outputs[:,:,:-1]\n",
        "                if loss_type == \"XE\":\n",
        "                    loss = loss_fn(outputs_ignore_pad.view(-1,outputs_ignore_pad.size(-1)),labels.view(-1).to(device))\n",
        "                elif loss_type == \"BPR\" or loss_type == \"BPR_MAX\":\n",
        "                    loss = loss_fn(outputs,labels.to(device),x_lens,uid)\n",
        "            else:\n",
        "                if loss_type == \"XE\":\n",
        "                    loss = loss_fn(outputs.view(-1,outputs.size(-1)),labels.view(-1).to(device))\n",
        "                elif loss_type == \"BPR\" or loss_type == \"BPR_MAX\":   \n",
        "                    loss = loss_fn(outputs,labels.to(device),x_lens,uid)\n",
        "\n",
        "            loss.backward()\n",
        "        \n",
        "        \n",
        "            if train_method != \"normal\":\n",
        "                if train_method == \"interleave\":\n",
        "                    # interleave on the epochs\n",
        "                    if (j+1) % 2 == 0:\n",
        "                        optimizer_features.step()\n",
        "                    else:\n",
        "                        optimizer_ids.step()\n",
        "\n",
        "                elif train_method == \"nate\":\n",
        "                    if (epoch+1) % 2 == 0:\n",
        "                        optimizer_features.step()\n",
        "                    else:\n",
        "                        optimizer_ids.step()\n",
        "                    \n",
        "            else:\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.detach().cpu().item()\n",
        "\n",
        "        del outputs\n",
        "    \n",
        "        if torch.cuda.is_available(): \n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "    \n",
        "        training_hit, training_ndcg, training_mrr = Recall_Object(model,train_dl,\"train\")\n",
        "        validation_hit, validation_ndcg, validation_mrr = Recall_Object(model,val_dl,\"validation\")\n",
        "        testing_hit, testing_ndcg, testing_mrr = Recall_Object(model,test_dl,\"test\")\n",
        "    \n",
        "        # Record the best metrics that our model obtained\n",
        "        max_train_ndcg, max_val_ndcg, max_test_ndcg = record_best_tuple(\n",
        "            max_train_ndcg, max_val_ndcg, max_test_ndcg, \n",
        "            training_ndcg, validation_ndcg, testing_ndcg)\n",
        "        max_train_hit, max_val_hit, max_test_hit = record_best_tuple(\n",
        "            max_train_hit, max_val_hit, max_test_hit, \n",
        "            training_hit, validation_hit, testing_hit)\n",
        "        if validation_mrr > max_val_mrr: \n",
        "            max_train_mrr = training_mrr\n",
        "            max_val_mrr = validation_mrr\n",
        "            max_test_mrr = testing_mrr\n",
        "    \n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "        print(\"Training Loss: {:.5f}\".format(running_loss/len(train_dl)))\n",
        "        print(\"Train Hits \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*training_hit))\n",
        "        print(\"Train ndcg \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*training_ndcg))\n",
        "        print(\"Valid Hits \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*validation_hit))\n",
        "        print(\"Valid ndcg \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*validation_ndcg))\n",
        "        print(\"Test Hits \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*testing_hit))\n",
        "        print(\"Test ndcg \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*testing_ndcg))\n",
        "\n",
        "        \n",
        "    print(\"=\"*100)\n",
        "    print(\"Maximum Training Hit \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*max_train_hit))\n",
        "    print(\"Maximum Validation Hit \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*max_val_hit))\n",
        "    print(\"Maximum Testing Hit \\t @10: {:.5f} \\t @5 : {:.5f} \\t @1 : {:.5f}\".format(*max_test_hit))\n",
        "    return ((max_train_hit, max_val_hit, max_test_hit), \n",
        "            (max_train_ndcg, max_test_ndcg, max_val_ndcg), \n",
        "            (max_train_mrr, max_test_mrr, max_val_mrr))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyNVFPIrlUOg"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0Sc-6DqlUOh",
        "outputId": "8e683b91-134a-4ba7-892f-4ba6c0beb18c"
      },
      "source": [
        "import json\n",
        "\n",
        "for num_epochs in num_epochs_all:\n",
        "  for lr, lr_alt in zip(lr_all, lr_alt_all):\n",
        "    for reg, bpr_reg in zip(reg_all, bpr_reg_all):\n",
        "      for num_neg_samples in num_neg_samples_all:\n",
        "        for train_method in train_method_all: \n",
        "          for hidden_dim in hidden_dim_all: \n",
        "            for embedding_dim in embedding_dim_all:\n",
        "              for bert_dim in bert_dim_all: \n",
        "                for freeze_plot in freeze_plot_all: \n",
        "                  for loss_type in loss_type_all: \n",
        "                    for dilations in dilations_all: \n",
        "                      for tied in tied_all: \n",
        "                        model = initialize_model(\n",
        "                            model_type=model_type,\n",
        "                            device=device, \n",
        "                            embedding_dim=embedding_dim,\n",
        "                            hidden_dim=hidden_dim, \n",
        "                            output_dim=output_dim,\n",
        "                            genre_dim=genre_dim,\n",
        "                            batch_first=True,\n",
        "                            max_length=max_length,\n",
        "                            pad_token=pad_token,\n",
        "                            pad_genre_token=pad_genre_token,\n",
        "                            bert_dim=bert_dim,\n",
        "                            tied = tied,\n",
        "                            dropout=dropout)\n",
        "\n",
        "                        loss_function = initialize_loss_function(loss_type, num_neg_samples, bpr_reg)\n",
        "                        assert loss_function is not None\n",
        "\n",
        "                        ((max_train_hit, max_val_hit, max_test_hit), \n",
        "                          (max_train_ndcg, max_test_ndcg, max_val_ndcg), \n",
        "                          (max_train_mrr, max_test_mrr, max_val_mrr)) = (\n",
        "                             train_model(model, num_epochs, loss_function, loss_type, train_method, tied, \n",
        "                                         lr, lr_alt, reg))\n",
        "\n",
        "\n",
        "                        row_params = {\n",
        "                          'num_epochs': num_epochs,\n",
        "                          'lr': lr, \n",
        "                          'lr_alt': lr_alt, \n",
        "                          'reg': reg,\n",
        "                          'bpr_reg': bpr_reg,\n",
        "                          'train_method': train_method,\n",
        "                          'hidden_dim': hidden_dim,\n",
        "                          'embedding_dim': embedding_dim,\n",
        "                          'bert_dim': bert_dim,\n",
        "                          'max_length': max_length,\n",
        "                          'freeze_plot': freeze_plot,\n",
        "                          'loss_type': loss_type,\n",
        "                          'dilations': dilations\n",
        "                        }\n",
        "\n",
        "                        \n",
        "                        row_results = { \n",
        "                          'max_train_hit': max_train_hit,\n",
        "                          'max_val_hit': max_val_hit,\n",
        "                          'max_test_hit': max_test_hit, \n",
        "                          'max_train_ndcg': max_train_ndcg, \n",
        "                          'max_test_ndcg': max_test_ndcg, \n",
        "                          'max_val_ndcg': max_val_ndcg, \n",
        "                          'max_train_mrr': max_train_mrr, \n",
        "                          'max_test_mrr': max_test_mrr, \n",
        "                          'max_val_mrr': max_val_mrr, \n",
        "                        }\n",
        "                        row_entry = { \n",
        "                            'params': row_params, \n",
        "                            'results': row_results\n",
        "                        }\n",
        "                        with open(\"/content/gdrive/My Drive/Colab Notebooks/hyperparam-results.txt\", 'a') as f: \n",
        "                          json.dump(row_entry, f)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3706\n",
            "3706\n",
            "3706\n",
            "3706\n",
            "3706\n",
            "3706\n",
            "3706\n",
            "3706\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}